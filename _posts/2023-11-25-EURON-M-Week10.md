---
title: Deep Learning Specialization 2-6 | Batch Normalization
author: Su
date: 2023-11-25 05:55:00 +0800
categories: [DL]
tags: [EURON]
pin: false
use_math: true
---

Learning Source
+ [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning?utm_source=deeplearningai&utm_medium=institutions&utm_campaign=SocialYoutubeDLSC1W1L1#courses)
+ [부스트코스 딥러닝 2단계: 심층 신경망 성능 향상시키기](https://m.boostcourse.org/ai216/lectures/132205)

<br>

## **Normalizing activations in a network**
+ 배치 정규화 장점:
  + 하이퍼파라미터 탐색을 쉽게 만들어 준다
  + 신경망과 하이퍼파라미터의 상관관계를 줄여 준다.
+ 보통 활성화 함수 이전에 사용되며, 작동원리는 아래와 같다.
  + $ \mu = \frac{1}{m}\sum_i z^{(i)} $
  + $ \sigma^2 = \frac{1}{m}\sum_i (z^{(i)}-\mu)^2$
  + $ z_{norm}^{(i)} = \frac{z^{(i)}-\mu}{\sqrt{\sigma^2+\epsilon}} $
  + $ \tilde z^{(i)} = \gamma z^{(i)}_{norm} + \beta $
+ $ \gamma $ 와 $\beta$: 학습과정에서 학습하는 파라미터
  + 정규화 이후 다시 선형변환하는 이유: 항상 같은 분포 값을 갖지 않게 하기 위함

+ 만약 $w^{[3]}, b^{[3]}$ 라는 parameter를 학습시킨다면, $z^{[2]}$ 의 평균과 분산을 정규화하는 것이 더 효율적이다. ➡️ 배치 정규화
+ 하지만 은닉 유닛이 항상 평균 0, 표준편차 1을 갖는 것이 좋지만은 않다. 은닉 유닛은 다양한 분포를 가져야 하기 때문 ➡️ tilde
+ 경사하강법, 모멘텀, adam 등으로 감마랑 beta 업데이트 시킬 수 잇음
+ 감마랑 베타로 평균과 분산을 원하는 대로 설정 가능
+ 배치화는 입력층만 정규화하는 게 아니라 은닉층의 값들까지 정규화하는 것
  + 은닉 유닛의 평균과 분산을 정규화하는 거. 이때 0, 1로 고정X, 널은 범위를 갖게 해 비선형성을 살릴 수 있도록 하자. 


## **Fitting Batch Norm into a neural network**
+ 2 Steps of Hidden Layer
    + 첫째, 선형결합인 z 를 계산 ➡️ 배치 정규화
    + 둘째, 정규화 된 값들을 활성화 함수를 거쳐 활성화 값 a 를 얻는다.
+ 선형결합 단계에서 상수항 b 는 없어짐. 그 이유는 배치 정규화 과정에서 z의 평균을 빼주면 사라지기 때문
+ 해당 미니 배치 안의 데이터만 사용히지
+ 배치 정규화는 미니 배치를 보고 $Z^{[L]}$ 이 평균 0, 분산 1을 갖도록 정규화한 뒤 베타랑 감마로 조정


## **Why does Batch Norm work?**
+ 배치 정규화는 입력특성  $X$  의 평균을 0, 분산을 1로 만듦으로써 학습 속도를 빠르게 힌디/
+ 배치 정규화가 잘 되는 이유중 하나는 이전 층의 가중치 영향을 덜 받기 때문
  + 은닉층 값의 분포 변화를 줄여줘서, 입력 값의 분포를 제한
  + 즉, 배치 정규화는 입력값이 바뀌어서 발생하는 문제를 안정화시킴
  + 앞층과 뒷층의 매개변수의 상관 관계를 줄여주기 때문에, 학습속도를 향상시킬 수 있다.
+ 배치 정규화의 또 다른 효과: 파라미터의 정규화(regularization)
  + 미니배치로 계산한 평균과 분산은 전체 데이터의 일부으로 추정한 것이기 때문에 잡음이 끼어있습니다.
+ 드롭아웃의 경우 은닉유닛에 확률에 따라 0 혹은 1을 곱함 ➡️ 곱셈 잡음
+ 배치 정규화의 경우 곱셈잡음( $\times \frac{1}{\sigma} $)과 덧셈 잡음( $-\mu$ )이 동시에 있다. ➡️ 약간의 정규화 효과
+ 은닉층에 잡음을 추가한다는 것 = 이후 은닉층이 하나의 은닉 유닛에 너무 의존하지 않도록 
+ 은닉층에 잡음을 추가한다는 것 = 이후 은닉층이 하나의 은닉 유닛에 너무 의존하지 않도록 
+ 큰 미니배치를 사용시 이 정규화 효과는 상대적으로 약해진다.
+ 공변량 변화: 데이터 분포가 변화하는 것

+ 깊은 층의 가중치가 1번 층처럼 앞쪽 층의 가중치의 변화에 영향을 덜 받는다
+ 배치 정규화는 이전 층에서 들어오는 은닉 분포의 변화를 줄여 준다.

+ 드롭아웃: 큰 미니배치를 사용하면 일반화 효과가 줄어든다. -> 배치 정규화로 일반화시키려고 하지 마라. 
  + 학습 속도 올리는 데에 사용해야 함

## **Batch Norm at test time**
+ 테스트 시에는 배치가 하나이기 때문에 평균과 분산을 계산할 수 없다. 따라서, 학습시에 사용된 미니배치들의 지수 가중 이동 평균을 추정치로 사용한다.
  + 독립된 $\mu$ 와 $sigma^2$ 추정치 사용 <- 지수가중평균
+ 테스트할 때는 한 번에 sample 하나를 처리해야 한다. 