---
title: Deep Learning Week4 - Information Theory
author: Sujin Kim
date: 2023-05-08 03:55:00 +0800
categories: [MLDL]
tags: [ECC, DL]
pin: false
use_math: true

---
# 1. Amount of Information
**Discrete random variable(r.v.) $x$ì˜ ì •ë³´ëŸ‰**(amount of information, ë˜ëŠ” degree of surprise)ì€ ì–´ë–»ê²Œ ì¸¡ì •í•  ìˆ˜ ìˆì„ê¹Œ? ì´ëŸ¬í•œ ì •ë³´ëŸ‰ì„ $h(x)$ë¼ ë†“ì. <br><br>
ì–´ë–¤ ì‚¬ê±´ì´ ì‹¤ì œë¡œ ì¼ì–´ë‚  í™•ë¥ ì´ ë‚®ì€ë° ìš°ë¦¬ê°€ ê·¸ ì‚¬ê±´ì„ ê´€ì¸¡í–ˆë‹¤ê³  í•˜ë©´, ì •ë³´ëŸ‰ì´ í¬ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤. ë‹¤ë¥¸ ë§ë¡œ  $p(X=a)$ê°€ ë‚®ì€ë° ìš°ë¦¬ê°€ $x=a$ë¥¼ ê´€ì¸¡í–ˆë‹¤ê³  í•˜ë©´, $h(x)$ê°€ í¬ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ í•­ìƒ 100ì ì„ ë§ëŠ” ì¹œêµ¬ê°€ ë˜ 100ì ì„ ë§ëŠ” ê²ƒê³¼ 50ì ì„ ë°›ëŠ” ê²ƒ ì¤‘, í›„ìë¥¼ ë³´ëŠ” ê²ƒì´ ë” ì •ë³´ëŸ‰ì´ í´ ê²ƒì´ë‹¤. ì´ëŸ¬í•œ ì´ìœ ë¡œ 1ï¸âƒ£ **$h(x)$ëŠ” $p(x)$ì— ì˜í–¥ì„ ë°›ì„ ìˆ˜ ë°–ì— ì—†ë‹¤**. <br>
<br> ë³€ìˆ˜ë¥¼ í•˜ë‚˜ ë” ì¶”ê°€í•˜ì—¬, independent r.v. $x$ì™€ $y$ì— ëŒ€í•´ ìƒê°í•´ë³´ì. ì´ë•Œ $x$ì™€ $y$ì˜ joint probabilityëŠ” $p(x, y)=p(x)p(y)$ì´ë‹¤. ì •ë³´ëŸ‰ì˜ ê²½ìš° $x$ì™€ $y$ê°€ ì„œë¡œ ë…ë¦½ì´ë¯€ë¡œ ê°ê° ë”í•´ì£¼ë©´ ëœë‹¤.<br>2ï¸âƒ£ **ì¦‰, $h(x, y) = h(x)+h(y)$ì´ë‹¤**.

# 2. Discrete Variable Entropy
ì´ëŸ¬í•œ ì‹ 1ï¸âƒ£,2ï¸âƒ£ë¥¼ ëª¨ë‘ ë§Œì¡±í•˜ëŠ” ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. <br><br>$h(x)=-\log_2p(x)$<br> <br>
$h(x)$ëŠ” 0 ë˜ëŠ” ì–‘ìˆ˜ê°€ ë˜ë©°, $p(x)$ê°€ ë‚®ì„ìˆ˜ë¡ ì»¤ì§„ë‹¤. ë˜í•œ ë°‘ì´ ë°˜ë“œì‹œ 2ì¼ í•„ìš”ëŠ” ì—†ë‹¤. ë°‘ ë³€í™˜ì„ í•  ê²½ìš° rescale ì—¬ë¶€ê°€ ë‹¬ë¼ì§ˆ ë¿ì´ë‹¤. ë°‘ì´ 2ì¼ ë•Œ ì •ë³´ëŸ‰ì˜ ë‹¨ìœ„ëŠ”bit, eì¼ ë•Œ nat(natural unit)ì´ë‹¤. ì´ë•Œ **í•œ ì‚¬ê±´ì— ëŒ€í•œ ìê¸° ìì‹ ì˜ ì •ë³´ëŸ‰**ì„ ë‚˜íƒ€ë‚¸ë‹¤ê³  í•´ì„œ **Self-Information**ì´ë¼ê³ ë„ í•œë‹¤. <br>
<br>**entropy**ëŠ” **ì–´ë–¤ ë‹¤ìˆ˜ì˜ ì‚¬ê±´ì— ëŒ€í•œ ê°ê°ì˜ ì •ë³´ëŸ‰ì˜ ê¸°ëŒ“ê°’**ì´ë‹¤. **r.v.ì—ì„œ ê¸°ëŒ€ë˜ëŠ” ì •ë³´ëŸ‰**ì´ê¸°ë„ í•˜ë‹¤. ì´ë¥¼ ê³„ì‚°í•´ë³´ì.  <br>ìš°ë¦¬ê°€ ì•„ëŠ” ê¸°ëŒ“ê°’ ê³µì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.<br> <br>
$E(x) = \sum xp(x)$
<BR> <br>$x$ ëŒ€ì‹  $h(x)$ì„ ëŒ€ì…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.
<br> <br>$E(h(x)) = \sum h(x)p(x)$
<br> <br>ë”°ë¼ì„œ ì •ë³´ëŸ‰ì˜ ê¸°ëŒ“ê°’ $H(x)$ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
<br> <br>$H(x) = E(h(x)) = - \sum_x p(x)log_2p(x)$<br> <br>
ì´ëŸ¬í•œ ëª¨ë“  ì‚¬ê±´ ì •ë³´ëŸ‰ì˜ ê¸°ëŒ“ê°’ $H(x)$ë¥¼ **Shannon entropy**ë¼ê³  ë¶€ë¥¸ë‹¤. ì „ì²´ ì‚¬ê±´ í™•ë¥ ë¶„í¬ì— ëŒ€í•œ ë¶ˆí™•ì‹¤ì„±ì˜ ì–‘ì„ ë‚˜íƒ€ë‚¼ ë•Œ ì“°ì¸ë‹¤.
<br> <br>
ìš°ë¦¬ëŠ” ë¡œê·¸í•¨ìˆ˜ì˜ ì§„ìˆ˜ëŠ” 0ë³´ë‹¤ ì»¤ì•¼í•œë‹¤ê³  ë°°ì› ë‹¤. ê·¸ë ‡ë‹¤ë©´ $p(x)=0$ì¸ ìƒí™©ì—ì„œ $H(x)$ëŠ” ì–´ë–»ê²Œ ë ê¹Œ? <br>$\lim_{p \to 0^+} p\ln p = 0$ì´ë¯€ë¡œ $p(x)=0$ì¼ ë•Œ $H(x)=0$ì´ ëœë‹¤.
<br>
$H(x)$ëŠ” uniform distribution(ê· ì¼ë¶„í¬)ì¼ìˆ˜ë¡ ì»¤ì§€ê³ , non-uniform distribution(ë¹„ê· ì¼ë¶„í¬)ì¼ìˆ˜ë¡ ì‘ì•„ì§„ë‹¤. ì˜ˆì‹œë¡œ í™•ì¸í•´ë³´ì. <br><br>
ë¨¼ì € r.v. $x$ê°€ 8 possible statesë¥¼ ê°–ê³ , ê°ê°ì˜ í™•ë¥ ì´ ëª¨ë‘ ê°™ì€ ë•Œì˜ entropyëŠ” ì–´ë–»ê²Œ ë ê¹Œ?  $H(x) = -8 \times \frac{1}{8}\log_2{\frac{1}{8}} = 3$ì´ ë  ê²ƒì´ë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” 3 bits ê¸¸ì´ì˜ messageë¥¼ ì „ë‹¬(transmit)í•´ì•¼ í•œë‹¤.<br><br>
ê·¸ë ‡ë‹¤ë©´ r.v. $x$ê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê°’ì´ ${a, b, c, d, e, f, g, h}$(8 possible states)ì´ê³ , ê°ê°ì˜ í™•ë¥ ê°’ì´ $( \frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{16}, \frac{1}{64}, \frac{1}{64}, \frac{1}{64}, \frac{1}{64})$ë¼ë©´ ì´ë•Œ entropyëŠ” ì–´ë–»ê²Œ ë ê¹Œ?<br>ê³„ì‚°í•˜ë©´ ê²°ê³¼ëŠ” 2ê°€ ë‚˜ì˜¨ë‹¤. a, b, c, .., hëŠ” ê°ê° 0(a), 10(b), 110, 1110, 111100, 111101, 111110, 111111ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. The average length of the codeëŠ”  $\frac{1}{2}  \times 1+ \frac{1}{4}  \times 2+\frac{1}{8}  \times 3+\frac{1}{16}  \times 4+4 \times  \frac{1}{64}  \times 6 = 2$bitsì´ë‹¤. <br><br>ì´ë•Œ a, b, c, d...ë¥¼ 0, 10, 01, 11...ë¡œ í‘œí˜„í•˜ì§€ ì•ŠëŠ” ì´ìœ ë¥¼ ì•Œì•„ë³´ì. 0, 10, 01, 11...ë¡œ í‘œí˜„í•œë‹¤ë©´ 0110ì€ adaì¼ê¹Œ? cdì¼ê¹Œ? ì´ëŸ° ì´ìœ  ë•Œë¬¸ì— 0, 10, 01, 11...ë¡œ í‘œí˜„í•˜ì§€ ì•ŠëŠ”ë‹¤.
<br><br>
ë°©ê¸ˆ ì˜ˆì‹œì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë“¯ **entropyëŠ” Non-uniform distributionë³´ë‹¤ uniform distributionì—ì„œ ë” ì»¤ì§„ë‹¤**.
<br>Shannonì€ Noiseless coding theoremì—ì„œ 'The entropy is a lower bound on the number of bits needed to transmit the state of a r.v.'ì„ì„ ë³´ì˜€ë‹¤. ì¦‰, entropyëŠ” í•´ë‹¹ r.v.ë¥¼ encodinigí•˜ëŠ” ë°ì— í•„ìš”í•œ í‰ê·  ì •ë³´ëŸ‰(ë‹¨ìœ„ bit)ì˜ lower boundì´ë¼ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ entropyê°€ 2.8ì¸ ê²½ìš°, 3bitì˜ í¬ê¸°ë¥¼ ê°€ì§€ëŠ” ì €ì¥ ê³µê°„ì´ í•„ìš”í•˜ë‹¤.

## Entropyì˜ minmax

entropyì˜ ì‹ì„ ë‹¤ì‹œ ì ì–´ë³´ì.<br><br>
$H(x) = - \sum_x p(x)log_2p(x)$<br><br>
entropyì˜ ìµœì†Ÿê°’ì„ ì•Œì•„ë³´ì. ë¨¼ì € $0 \leq p(x) \leq 1$ì´ë¯€ë¡œ $H(x) \geq 0$ì´ë‹¤. í•˜ë‚˜ê°€ $p_i = 1$ì´ê³  ë‚˜ë¨¸ì§€ ëª¨ë‘ê°€ $p_{j \neq i}=0$ì¼ ë•Œ entropyê°€ 0ìœ¼ë¡œ ìµœì†Œì¸ ê²ƒì´ë‹¤.<br><br>
ê·¸ë ‡ë‹¤ë©´ ìµœëŒ“ê°’ì€ ì–´ë–»ê²Œ êµ¬í•  ìˆ˜ ìˆì„ê¹Œ? Lagrange multiplier(ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²•)ì„ ì´ìš©í•˜ì. ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²•ì€ ì œì•½ì¡°ê±´ í•˜ì—ì„œ ë‹¤ë³€ìˆ˜í•¨ìˆ˜ì˜ ìµœëŒ€, ìµœì†Œë¥¼ êµ¬í•˜ê¸° ìœ„í•œ ë°©ë²•ì´ë‹¤. <br>
> Lagrange Multiplier(ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²•)
	- Maximize $f(x, y)$ s.t. $g(x, y)=0$
	- Lagrange function: $L(x, y, \lambda) = f(x, y) - \lambda g(x, y)$
	- Solve $\nabla_{x, y, \lambda} L(x, y, \lambda) = 0$

<br>Lagrange function: $\hat{H} = - \sum_i p(x_i)log_2p(x_i) + \lambda ( \sum_i p(x_i)-1)$<br><br>
í•¨ìˆ˜ $\hat{H}$ ì— ëŒ€í•´ ì•„ë˜ ë‘ ì‹ì„ ë§Œì¡±í•˜ëŠ” ì ì´ ìµœëŒ€ ë˜ëŠ” ìµœì†Œì˜ í›„ë³´ê°€ ëœë‹¤.
<br>1ï¸âƒ£ $\nabla \hat{H} = 0$
<br>2ï¸âƒ£ $\hat{H}_{\lambda} = 0$<br><br>
ì‹ 1ï¸âƒ£ì—ì„œ $\frac{\partial  \hat{H}}{\partial p(x_i)} = - \log p(x_i) -1+\lambda = 0$<br>
ì‹ 2ï¸âƒ£ì—ì„œ $\frac{\partial  \hat{H}}{\partial  \lambda} = \sum_i p(x_i) -1= 0$<br>
ë”°ë¼ì„œ $p(x_i) = \exp(-1+\lambda)$ where $\sum_i p(x_i) =1$ì´ë¯€ë¡œ<br> $p(x_i) = \frac{1}{M}$ where M is the total number of states $x_i$ì¼ ë•Œ entropyê°€ ìµœëŒ€ì´ë‹¤. ì¦‰ discreteí•œ ìƒí™©ì—ì„œëŠ” uniformí•  ë•Œ entropyê°€ ì œì¼ ë†’ë‹¤.<br>
í•˜ì§€ë§Œ ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²•ìœ¼ë¡œëŠ” ìµœëŒ€ì¸ì§€ ìµœì†Œì¸ì§€ í™•ì‹ í•˜ì§€ ëª»í•œë‹¤. ë”°ë¼ì„œ entropyì˜ second derivativeë¥¼ í™•ì¸í•´ë³´ì•„ì•¼ í•œë‹¤. ì•„ë˜ ì‹ì—ì„œ ìŒìˆ˜(ìœ„ë¡œ ë³¼ë¡)ê°€ ë‚˜ì˜¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. <br> $\frac{\partial  \hat{H}}{\partial p(x_i)p(x_j)} = -I_{ij}  \frac{1}{p_i}$ where $I_{ij}$ are the elements of the identity matrix<br>
 entropyì˜ ìµœëŒ“ê°’ì€ $\log M$($\sum_i p(x_i) =1$)ì´ë‹¤. 

## Another view
entropyë¥¼ ë°”ë¼ë³´ëŠ” ë‹¤ë¥¸ ê´€ì ë„ ì¡´ì¬í•œë‹¤. ë¨¼ì € Nê°œì˜ ë™ì¼í•œ ì‚¬ë¬¼ì´ ìˆê³ , ì´ë¥¼ $i$ë²ˆì§¸ bin(ë°”êµ¬ë‹ˆ)ì— $n_i$ê°œë¥¼ ë„£ëŠ”ë‹¤ê³  ê°€ì •í•˜ì. ê·¸ëŸ¬ë©´ ì´ ê°€ëŠ¥í•œ ê°€ì§“ìˆ˜ëŠ” $W = \frac{N!}{\prod_i n_i!}$ê°€ ëœë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë™ì¼í•œ ê³µ 9ê°œë¥¼ 1ë²ˆì§¸ ë°”êµ¬ë‹ˆì— 2ê°œ, 2ë²ˆì§¸ì— 3ê°œ, 3ë²ˆì§¸ì— 4ê°œì”© ë„£ëŠ”ë‹¤ê³  í•˜ì. ê·¸ëŸ¬ë©´ ì´ ê²½ìš°ì˜ ìˆ˜ëŠ”  $\frac {C_9^2 C_7^3 C_4^4}{3!}=\frac{9!}{2!3!4!}$ê°€ ëœë‹¤.
> $\left(\begin{array}{l} n \\ p \end{array}\right) \text = C_n^p =\frac{n(n-1)(n-2) \cdots(n-p+1)}{p !}=\frac{n !}{p !(n-p) !}$


<center><img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/6ce1fbf5-8cd7-4856-8255-e9410accc823" width="80%" height="50%">

<br>ë‹¤ì‹œ ëŒì•„ì˜¤ë©´, $H = \frac{1}{N}  \ln W = \frac{1}  {N}  \ln N! - \frac  {1}{N}  \sum_i \ln{n_i}!$ì´ ëœë‹¤. ë§Œì•½ $N$ â†’ $\infin$ë¼ê³  í•œë‹¤ë©´, Stirlingâ€™s approximationì— ì˜í•´ $\ln N! \approx N \ln N - N$ê°€ ëœë‹¤. ë”°ë¼ì„œ $H = - \lim_{N \to  \infin}  \sum_i (\frac  {n_i}{N}  \ln (\frac  {n_i}{N}) = \sum_i p_i \ln p_i$ê°€ ëœë‹¤. ì—¬ê¸°ì„œ Entropyê°€ ìµœì†Ÿê°’ì„ ê°€ì§ˆ ë•ŒëŠ” $p_i = 1$ì´ë©°, $p_{j \neq i} = 0$ì´ê³ , ìµœëŒ“ê°’ì„ ê°€ì§ˆ ë•ŒëŠ”  $p_i = \frac  {1}  {M}$ ($M$: bin ê°œìˆ˜)ì´ë‹¤.


## Cross Entropy
Entropyê°€ ì •ë‹µì´ ë‚˜ì˜¬ í™•ë¥ (p)ë§Œì„ ëŒ€ìƒìœ¼ë¡œ ì¸¡ì •í•œ ê°’ì´ì—ˆë‹¤ë©´, Cross EntropyëŠ” ëª¨ë¸ì—ì„œ ì˜ˆì¸¡í•œ í™•ë¥ (q)ì™€ ì •ë‹µì´ ë‚˜ì˜¬ í™•ë¥ (p)ë¥¼ ëª¨ë‘ ì‚¬ìš©í•œ ê°’ì´ë‹¤. **Cross Entropy**ëŠ” **ëª¨ë¸ì—ì„œ ì˜ˆì¸¡í•œ í™•ë¥  ê°’ì´ ì‹¤ì œ í™•ë¥ ê³¼ ë¹„êµí–ˆì„ ë•Œ í‹€ë¦´ ìˆ˜ ìˆëŠ” ì •ë³´ëŸ‰**ì´ë‹¤. ì‘ì„ìˆ˜ë¡ ê·¸ ëª¨ë¸ì´ ë” ì˜ˆì¸¡ì„ ì˜ í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. 
$H(p, q) = -E_p[\log q]$
$H(p, q) = - \sum_x p(x) \log q(x)$<br>
ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì†ì‹¤í•¨ìˆ˜ë¡œ ë§ì´ ì‚¬ìš©ëœë‹¤. í•™ìŠµ ê³¼ì •ì—ì„œ ì˜ˆì¸¡ê°’ê³¼ ì •ë‹µê°’ì˜ cross entropy ê°’ì„ ì¤„ì´ê¸° ìœ„í•´ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì—…ë°ì´íŠ¸í•œë‹¤.

# 3. Continuous Variable Entropy

EntropyëŠ” **continuous random variable**ì— ëŒ€í•´ì„œë„ ì •ì˜ ê°€ëŠ¥í•˜ë‹¤. ì´ ê²½ìš°ì—ëŠ” entropyë¥¼ differential entropyë¼ê³  ë¶€ë¥´ê¸°ë„ í•œë‹¤.<br>
$H(x) = - \int p(x) \ln p(x) dx$<br>

ì˜ˆë¥¼ ë“¤ì–´ continuous r.v. $x$ê°€ Gaussian distributionì„ ë”°ë¥¸ë‹¤ê³  í•˜ì.
$p(x) = \frac  {1}{(2 \pi  \sigma^2) ^{1/2}  }  \exp(- \frac  {(x-\mu )^2 }{2 \sigma^2})$
$\ln p(x) = - \frac{1}{2}  \ln 2 \pi  \sigma^2 - \frac  {(x-\mu)^2}{2 \sigma^2}$
$p(x) \ln p(x) = \frac  {1}{(2 \pi  \sigma^2)^{1/2}}  \exp({-\frac{(x-\mu)^2}{2 \sigma^2}})(-\frac{1}{2}  \ln  {2\pi  \sigma^2} - \frac{(x-\mu)^2}{2 \sigma^2})$
$- \int p(x) \ln p(x) dx = - \{$ 1ï¸âƒ£ $\int \frac  {1}{(2 \pi  \sigma^2)^{1/2}}  \exp({-\frac{(x-\mu)^2}{2 \sigma^{2}}})(- \frac{1}{2}  \ln  {2\pi  \sigma^2})dx-$ 2ï¸âƒ£ $\int \frac  {1}{(2 \pi  \sigma^2)^{1/2}}  \exp({-\frac{(x-\mu)^2}{2 \sigma^2}})(\frac{(x-\mu)^2}{2 \sigma^2})dx \}$
1ï¸âƒ£ $\int \frac  {1}{(2 \pi  \sigma^2)^{1/2}}  \exp({-\frac{(x-\mu)^2}{2 \sigma^{2}}})(- \frac{1}{2}  \ln  {2\pi  \sigma^2})dx = - \frac{1}{2}  \ln 2 \pi  \sigma^2$
2ï¸âƒ£ $\int \frac  {1}{(2 \pi  \sigma^2)^{1/2}}  \exp({-\frac{(x-\mu)^2}{2 \sigma^2}})(\frac{(x-\mu)^2}{2 \sigma^2})dx = - \frac {1}{2 \sigma^2} \int(x-\mu)^2\frac {1}{(2 \pi  \sigma^2)^{1/2}}  \exp({-\frac{(x-\mu)^2}{2 \sigma^{2}}})dx = - \frac {1}{2}$


ì´ë•Œ $E[(x-\mu)^2] = \sigma^2$ëŠ” varianceì´ë‹¤. ë”°ë¼ì„œ $- \int p(x) \ln p(x) dx = \frac  {1}{2}  \ln  {2 \pi  \sigma^2} + \frac{1}{2}$ê°€ ëœë‹¤.
$\sigma^2$ì´ ì»¤ì§ˆìˆ˜ë¡(broader, uniformì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡), entropyëŠ” ë”ìš± ì»¤ì§„ë‹¤. entropyëŠ” ìŒìˆ˜ë„ ë  ìˆ˜ ìˆë‹¤. $(\sigma^2 < \frac  {1}{2 \pi e})$

ê·¸ë ‡ë‹¤ë©´ í‰ê·  $\mu$ì™€ ë¶„ì‚° $\sigma^2$ì´ ì •í•´ì ¸ ìˆê³ , ì„¸ ê°€ì§€ ì œì•½ ì¡°ê±´ì´ ìˆì„ ë•Œ differential entropyë¥¼ maximize í•´ë³´ì.
ì œì•½ ì¡°ê±´ ì„¸ ê°€ì§€ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
- $\int_{\infin}^{-\infin} p(x) dx = 1$
- $\int_{\infin}^{-\infin} xp(x) dx = \mu$
- $\int_{\infin}^{-\infin} (x-\mu)^2p(x) dx = \sigma^2$

$H(x) = - \int p(x) \ln p(x) dx$
Lagrange multipliersì™€ calculus of variationsë¥¼ ì´ìš©í•˜ë©´
$p(x) = \exp(-1+ \lambda_1 + \lambda_2 x + \lambda_3 (x-\mu)^2)$
$p(x) = \frac{1}{(2 \pi  \sigma^2)^{1/2}}  \exp (-\frac{(x-\mu)^2}{2\sigma^2})$

ë”°ë¼ì„œ entropyëŠ” $\frac  {1}{2}  \ln  {2 \pi  \sigma^2} + \frac{1}{2}$ê°€ ëœë‹¤. ì´ ê°’ì€ ìŒìˆ˜ë„ ë  ìˆ˜ ìˆë‹¤.

continuous r.v.ì˜ í‰ê· ê³¼ ë¶„ì‚°ì´ ì •í•´ì ¸ ìˆì„ ë•Œ entropyëŠ” Gaussian distributionë¥¼ ë”°ë¥¼ ë•Œ ìµœëŒ€ì´ë‹¤.
<br>

# 4. EntropyëŠ” ì–´ë””ì— ì“°ì¼ê¹Œ?
 ## MNIST Dataset
 -   handwritten digits
 - Training set: 60,000 examples
-   Test set: 10,000 examples
-   \# class: 10 â†’ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9
<center><img src=https://github.com/sml09181/sml09181.github.io/assets/105408672/afad4117-e5e6-439b-9206-a235af5e94b8 width="60%" height="60%">

ë§¤ìš° ì‰¬ìš´ datasetì´ë‹¤.
 -   Training Accuracy: 100%
 -   Test Accuracy: 99% â¬†ï¸

ë”°ë¼ì„œ ìš°ë¦¬ê°€ ìƒˆë¡œìš´ ëª¨ë¸ì„ ê°œë°œí•  ë•Œ, MNIST datasetì—ì„œ ì‹¤íŒ¨í•˜ë©´ ë‹¤ë¥¸ datasetì—ì„œë„ ì‹¤íŒ¨í•˜ê²Œ ëœë‹¤. ë§Œì•½ MNISTì—ì„œ ì„±ê³µí•´ë„ ë‹¤ë¥¸ datasetì—ì„œì˜ ì„±ê³µ ì—¬ë¶€ëŠ” ì¥ë‹´í•  ìˆ˜ ì—†ë‹¤.
<br>
<br>
<center><img src=https://github.com/sml09181/sml09181.github.io/assets/105408672/b4b8837a-2cd7-4a11-b395-1d9a33205148 width="95%">

Dataê°€ Neural Network(ì‚¬ì§„ ì† íšŒìƒ‰ íŒ)ì„ í†µí•˜ë©´ meaningful representationì´ ëœë‹¤. ì´ë•Œ ClassifierëŠ” outputìœ¼ë¡œ ê° classì˜ probabilityë¥¼ ë±‰ëŠ”ë‹¤.
   -   The probability of class 0: <span style="color:blueviolet">0.7</span>
   -   â€¦
   -   The probability of class 9: <span style="color:blue">0.1</span>
   - 
-   Model **confidence** can be measured by (í™•ì‹  ì •ë³´ë‹ˆê¹Œ uniformí•˜ì§€ X)
    -   $\max_ip_i$ for $i = 0, ..., 9$
    -   **Negative Entropy**
<br>

# 5. Conditional Entropy
-   Conditional Entropyë„ ì •ì˜í•  ìˆ˜ ìˆë‹¤.
    -   ì¦‰, $x$ì— ëŒ€í•œ ê²ƒì€ ì´ë¯¸ ì•Œê³  ìˆì„ ë•Œ, yì— ëŒ€í•œ entropy
        
        -   ì •ë³´ëŸ‰: $-\ln p(y|x)$
        -   ê·¸ë ‡ë‹¤ë©´, í‰ê· ì ì¸ ì •ë³´ëŸ‰ì„ ë‚˜íƒ€ë‚´ëŠ” entropy $H(y|x)$ì˜ ê²½ìš°ëŠ” ì–´ë–»ê²Œ ë ê¹Œ?
    -   $H(y|x) = - \int \int p(y, x) \ln p(y|x) dydx$
        
        -   NOTE) $H(x) = - \int p(x) \ln p(x) dx$
        -   Conditional entropy of $y$ given $x$
    -   $H(x, y) = H(y|x) + H(x)$
        -   $H(x, y)$: entropy of $p(x, y)$
        -   $H(x)$: entropy of $p(x)$
        -   ì¦‰, $x$ì™€ $y$ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì •ë³´ëŸ‰ = $x$ë¥¼ í‘œí˜„í•˜ê³ , $y$ given $x$ë¥¼ í‘œí˜„í•˜ëŠ” ì •ë³´ëŸ‰

# 6. KL Divergence
Kullback-Leibler divergenceì€ ìƒëŒ€ ì—”íŠ¸ë¡œí”¼(relative entropy), ì •ë³´ íšë“ëŸ‰(information gain), information divergenceì™€ ê°™ì€ ë§ì´ë‹¤.

-   ì§€ê¸ˆê¹Œì§€ ì •ë³´ëŸ‰ê³¼ ì—”íŠ¸ë¡œí”¼ì— ëŒ€í•´ ì‚´í´ë³´ì•˜ë‹¤.
    -   ìš°ë¦¬ëŠ” unknown distribution $p(x)$ì— ê´€ì‹¬ì´ ë§ë‹¤.
    -   í•˜ì§€ë§Œ, ëª¨ë¥´ê¸°ì— $q(x)$ë¡œ ëŒ€ì‹  ëª¨ë¸ë§í•˜ê³ ì í•œë‹¤.
    -   ê·¸ë•Œ, í‰ê· ì ìœ¼ë¡œ ë” í•„ìš”í•œ ì •ë³´ëŸ‰ì€ ì–´ë–»ê²Œ ë ê¹Œ?
    -   $KL(p||q) = - \int p(x) \ln q(x) dx - (- \int p(x) \ln p(x) dx) \\\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, = - \int p(x) \ln (\frac {q(x)}{p(x)}) dx$ (0 ì´ìƒ)
-   KL divergence
    -   ë“±í˜¸ê°€ ì„±ë¦½í•  ì¡°ê±´: $q(x)$ì™€ $p(x)$ê°€ ê°™ì„ ë•Œ
    -   $KL(p||q) \neq KL(q||p)$
    -   KL divergence > 0 â†’ why?
    - 
## Convex
xê°’ì„ interpolation / yê°’ì„ interpolation

expectationì´ ë¶€ë“±í˜¸ì™€ í•¨ê»˜ ë‚˜ê°”ë‹¤ ë“¤ì–´ì™”ë‹¤ í•´ë„ ëœë‹¤.
<br>
<br>
<br>
<br>

Further:

 - [ ] ì •ë³´ëŸ‰ì˜ rescaleì— ëŒ€í•´ ë‹¤ì‹œ ì°¾ì•„ë³´ê¸°
 - [ ] ê²°í•© í™•ë¥ , ì¡°ê±´ë¶€ í™•ë¥  ë“± ì •ë¦¬í•˜ê¸°
 - [ ] entropyì˜ ì¢…ë¥˜ ì•Œì•„ë³´ê¸° -> ì„€ë¨¼ ì—”íŠ¸ë¡œí”¼ë¥¼ ê·¸ëƒ¥ entropyë¡œ ë¶€ë¥´ëŠ” ê²ƒì¸ì§€

<br>
Reference:<br>
- [ì •ë³´ëŸ‰ê³¼ ì—”íŠ¸ë¡œí”¼ì˜ ì˜ë¯¸](https://bskyvision.com/entry/%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0-%EC%A0%95%EB%B3%B4%EB%9F%89%EA%B3%BC-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC%EC%9D%98-%EC%9D%98%EB%AF%B8) <br>
[ì •ë³´ì´ë¡ : ì—”íŠ¸ë¡œí”¼, KL-Divergence](https://reniew.github.io/17/) <br>
[Rì—ì„œ ì„€ë„Œ ì—”íŠ¸ë¡œí”¼(Shannon entropy) êµ¬í•˜ê¸°](https://m.blog.naver.com/pmw9440/221990235236) <br>
[Entropyë€(í‰ê· ì •ë³´ëŸ‰, ì •ë³´ëŸ‰ì˜ ê¸°ëŒ“ê°’)](https://dsaint31.tistory.com/entry/Math-Entropy-%EB%9E%80-%ED%8F%89%EA%B7%A0%EC%A0%95%EB%B3%B4%EB%9F%89-%EC%A0%95%EB%B3%B4%EB%9F%89%EC%9D%98-%EA%B8%B0%EB%8C%93%EA%B0%92) <br>
[Shannon Entropy](https://enfow.github.io/study/statistics/2020/02/06/shannon_entropy/) <br>
- [ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²• ì˜ˆì œğŸŒŸ](https://subprofessor.tistory.com/65) <br>
- [ì´ˆë³´ë¥¼ ìœ„í•œ ì •ë³´ì´ë¡  ì•ˆë‚´ì„œ - 1. Entropyë€ ë¬´ì—‡ì¼ê¹ŒğŸŒŸ](https://hyunw.kim/blog/2017/10/14/Entropy.html)
-[ì—”íŠ¸ë¡œí”¼ì™€ í¬ë¡œìŠ¤ì—”íŠ¸ë¡œí”¼](https://westshine-data-analysis.tistory.com/83#:~:text=%E2%96%B6%ED%81%AC%EB%A1%9C%EC%8A%A4%20%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC%EB%9E%80&text=%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC%EA%B0%80%20%EC%A0%95%EB%8B%B5%EC%9D%B4%20%EB%82%98%EC%98%AC,%EC%9E%88%EB%8A%94%20%EC%A0%95%EB%B3%B4%EB%9F%89%EC%9D%84%20%EC%9D%98%EB%AF%B8%ED%95%9C%EB%8B%A4.)

<kbd>hello</kbd>

<a>hello</a>

<code>hello</code>

<mark>hello</mark>

<under>hello</under>