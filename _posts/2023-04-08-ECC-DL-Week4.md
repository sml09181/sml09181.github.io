---
title: Deep Learning Week4 - Information Theory
author: Sujin Kim
date: 2023-05-08 03:55:00 +0800
categories: [MLDL]
tags: [ECC, DL]
pin: false
use_math: true

---
# 1. Amount of Information
**Discrete random variable(r.v.) $x$의 정보량**(amount of information, 또는 degree of surprise)은 어떻게 측정할 수 있을까? 이러한 정보량을 $h(x)$라 놓자. <br><br>
어떤 사건이 실제로 일어날 확률이 낮은데 우리가 그 사건을 관측했다고 하면, 정보량이 크다고 할 수 있다. 다른 말로  $p(X=a)$가 낮은데 우리가 $x=a$를 관측했다고 하면, $h(x)$가 크다. 예를 들어 항상 100점을 맞는 친구가 또 100점을 맞는 것과 50점을 받는 것 중, 후자를 보는 것이 더 정보량이 클 것이다. 이러한 이유로 1️⃣ **$h(x)$는 $p(x)$에 영향을 받을 수 밖에 없다**. <br>
<br> 변수를 하나 더 추가하여, independent r.v. $x$와 $y$에 대해 생각해보자. 이때 $x$와 $y$의 joint probability는 $p(x, y)=p(x)p(y)$이다. 정보량의 경우 $x$와 $y$가 서로 독립이므로 각각 더해주면 된다.<br>2️⃣ **즉, $h(x, y) = h(x)+h(y)$이다**.

# 2. Discrete Variable Entropy
이러한 식 1️⃣,2️⃣를 모두 만족하는 식은 다음과 같다. <br><br>$h(x)=-\log_2p(x)$<br> <br>
$h(x)$는 0 또는 양수가 되며, $p(x)$가 낮을수록 커진다. 또한 밑이 반드시 2일 필요는 없다. 밑 변환을 할 경우 rescale 여부가 달라질 뿐이다. 밑이 2일 때 정보량의 단위는bit, e일 때 nat(natural unit)이다. 이때 **한 사건에 대한 자기 자신의 정보량**을 나타낸다고 해서 **Self-Information**이라고도 한다. <br>
<br>**entropy**는 **어떤 다수의 사건에 대한 각각의 정보량의 기댓값**이다. **r.v.에서 기대되는 정보량**이기도 하다. 이를 계산해보자.  <br>우리가 아는 기댓값 공식은 다음과 같다.<br> <br>
$E(x) = \sum xp(x)$
<BR> <br>$x$ 대신 $h(x)$을 대입하면 다음과 같다.
<br> <br>$E(h(x)) = \sum h(x)p(x)$
<br> <br>따라서 정보량의 기댓값 $H(x)$는 다음과 같다.
<br> <br>$H(x) = E(h(x)) = - \sum_x p(x)log_2p(x)$<br> <br>
이러한 모든 사건 정보량의 기댓값 $H(x)$를 **Shannon entropy**라고 부른다. 전체 사건 확률분포에 대한 불확실성의 양을 나타낼 때 쓰인다.
<br> <br>
우리는 로그함수의 진수는 0보다 커야한다고 배웠다. 그렇다면 $p(x)=0$인 상황에서 $H(x)$는 어떻게 될까? <br>$\lim_{p \to 0^+} p\ln p = 0$이므로 $p(x)=0$일 때 $H(x)=0$이 된다.
<br>
$H(x)$는 uniform distribution(균일분포)일수록 커지고, non-uniform distribution(비균일분포)일수록 작아진다. 예시로 확인해보자. <br><br>
먼저 r.v. $x$가 8 possible states를 갖고, 각각의 확률이 모두 같은 때의 entropy는 어떻게 될까?  $H(x) = -8 \times \frac{1}{8}\log_2{\frac{1}{8}} = 3$이 될 것이다. 따라서 우리는 3 bits 길이의 message를 전달(transmit)해야 한다.<br><br>
그렇다면 r.v. $x$가 가질 수 있는 값이 ${a, b, c, d, e, f, g, h}$(8 possible states)이고, 각각의 확률값이 $( \frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{16}, \frac{1}{64}, \frac{1}{64}, \frac{1}{64}, \frac{1}{64})$라면 이때 entropy는 어떻게 될까?<br>계산하면 결과는 2가 나온다. a, b, c, .., h는 각각 0(a), 10(b), 110, 1110, 111100, 111101, 111110, 111111으로 표현할 수 있다. The average length of the code는  $\frac{1}{2}  \times 1+ \frac{1}{4}  \times 2+\frac{1}{8}  \times 3+\frac{1}{16}  \times 4+4 \times  \frac{1}{64}  \times 6 = 2$bits이다. <br><br>이때 a, b, c, d...를 0, 10, 01, 11...로 표현하지 않는 이유를 알아보자. 0, 10, 01, 11...로 표현한다면 0110은 ada일까? cd일까? 이런 이유 때문에 0, 10, 01, 11...로 표현하지 않는다.
<br><br>
방금 예시에서 확인할 수 있듯 **entropy는 Non-uniform distribution보다 uniform distribution에서 더 커진다**.
<br>Shannon은 Noiseless coding theorem에서 'The entropy is a lower bound on the number of bits needed to transmit the state of a r.v.'임을 보였다. 즉, entropy는 해당 r.v.를 encodinig하는 데에 필요한 평균 정보량(단위 bit)의 lower bound이라는 것이다. 예를 들어 entropy가 2.8인 경우, 3bit의 크기를 가지는 저장 공간이 필요하다.

## Entropy의 minmax

entropy의 식을 다시 적어보자.<br><br>
$H(x) = - \sum_x p(x)log_2p(x)$<br><br>
entropy의 최솟값을 알아보자. 먼저 $0 \leq p(x) \leq 1$이므로 $H(x) \geq 0$이다. 하나가 $p_i = 1$이고 나머지 모두가 $p_{j \neq i}=0$일 때 entropy가 0으로 최소인 것이다.<br><br>
그렇다면 최댓값은 어떻게 구할 수 있을까? Lagrange multiplier(라그랑주 승수법)을 이용하자. 라그랑주 승수법은 제약조건 하에서 다변수함수의 최대, 최소를 구하기 위한 방법이다. <br>
> Lagrange Multiplier(라그랑주 승수법)
	- Maximize $f(x, y)$ s.t. $g(x, y)=0$
	- Lagrange function: $L(x, y, \lambda) = f(x, y) - \lambda g(x, y)$
	- Solve $\nabla_{x, y, \lambda} L(x, y, \lambda) = 0$

<br>Lagrange function: $\hat{H} = - \sum_i p(x_i)log_2p(x_i) + \lambda ( \sum_i p(x_i)-1)$<br><br>
함수 $\hat{H}$ 에 대해 아래 두 식을 만족하는 점이 최대 또는 최소의 후보가 된다.
<br>1️⃣ $\nabla \hat{H} = 0$
<br>2️⃣ $\hat{H}_{\lambda} = 0$<br><br>
식 1️⃣에서 $\frac{\partial  \hat{H}}{\partial p(x_i)} = - \log p(x_i) -1+\lambda = 0$<br>
식 2️⃣에서 $\frac{\partial  \hat{H}}{\partial  \lambda} = \sum_i p(x_i) -1= 0$<br>
따라서 $p(x_i) = \exp(-1+\lambda)$ where $\sum_i p(x_i) =1$이므로<br> $p(x_i) = \frac{1}{M}$ where M is the total number of states $x_i$일 때 entropy가 최대이다. 즉 discrete한 상황에서는 uniform할 때 entropy가 제일 높다.<br>
하지만 라그랑주 승수법으로는 최대인지 최소인지 확신하지 못한다. 따라서 entropy의 second derivative를 확인해보아야 한다. 아래 식에서 음수(위로 볼록)가 나오는 것을 확인할 수 있다. <br> $\frac{\partial  \hat{H}}{\partial p(x_i)p(x_j)} = -I_{ij}  \frac{1}{p_i}$ where $I_{ij}$ are the elements of the identity matrix<br>
 entropy의 최댓값은 $\log M$($\sum_i p(x_i) =1$)이다. 

## Another view
entropy를 바라보는 다른 관점도 존재한다. 먼저 N개의 동일한 사물이 있고, 이를 $i$번째 bin(바구니)에 $n_i$개를 넣는다고 가정하자. 그러면 총 가능한 가짓수는 $W = \frac{N!}{\prod_i n_i!}$가 된다. 예를 들어 동일한 공 9개를 1번째 바구니에 2개, 2번째에 3개, 3번째에 4개씩 넣는다고 하자. 그러면 총 경우의 수는  $\frac {C_9^2 C_7^3 C_4^4}{3!}=\frac{9!}{2!3!4!}$가 된다.
> $\left(\begin{array}{l} n \\ p \end{array}\right) \text = C_n^p =\frac{n(n-1)(n-2) \cdots(n-p+1)}{p !}=\frac{n !}{p !(n-p) !}$


<center><img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/6ce1fbf5-8cd7-4856-8255-e9410accc823" width="80%" height="50%">

<br>다시 돌아오면, $H = \frac{1}{N}  \ln W = \frac{1}  {N}  \ln N! - \frac  {1}{N}  \sum_i \ln{n_i}!$이 된다. 만약 $N$ → $\infin$라고 한다면, Stirling’s approximation에 의해 $\ln N! \approx N \ln N - N$가 된다. 따라서 $H = - \lim_{N \to  \infin}  \sum_i (\frac  {n_i}{N}  \ln (\frac  {n_i}{N}) = \sum_i p_i \ln p_i$가 된다. 여기서 Entropy가 최솟값을 가질 때는 $p_i = 1$이며, $p_{j \neq i} = 0$이고, 최댓값을 가질 때는  $p_i = \frac  {1}  {M}$ ($M$: bin 개수)이다.


## Cross Entropy
Entropy가 정답이 나올 확률(p)만을 대상으로 측정한 값이었다면, Cross Entropy는 모델에서 예측한 확률(q)와 정답이 나올 확률(p)를 모두 사용한 값이다. **Cross Entropy**는 **모델에서 예측한 확률 값이 실제 확률과 비교했을 때 틀릴 수 있는 정보량**이다. 작을수록 그 모델이 더 예측을 잘 한다고 볼 수 있다. 
$H(p, q) = -E_p[\log q]$
$H(p, q) = - \sum_x p(x) \log q(x)$<br>
딥러닝 모델의 손실함수로 많이 사용된다. 학습 과정에서 예측값과 정답값의 cross entropy 값을 줄이기 위해 가중치와 편향을 업데이트한다.

# 3. Continuous Variable Entropy

Entropy는 **continuous random variable**에 대해서도 정의 가능하다. 이 경우에는 entropy를 differential entropy라고 부르기도 한다.<br>
$H(x) = - \int p(x) \ln p(x) dx$<br>

예를 들어 continuous r.v. $x$가 Gaussian distribution을 따른다고 하자.
$p(x) = \frac  {1}{(2 \pi  \sigma^2) ^{1/2}  }  \exp(- \frac  {(x-\mu )^2 }{2 \sigma^2})$
$\ln p(x) = - \frac{1}{2}  \ln 2 \pi  \sigma^2 - \frac  {(x-\mu)^2}{2 \sigma^2}$
$p(x) \ln p(x) = \frac  {1}{(2 \pi  \sigma^2)^{1/2}}  \exp({-\frac{(x-\mu)^2}{2 \sigma^2}})(-\frac{1}{2}  \ln  {2\pi  \sigma^2} - \frac{(x-\mu)^2}{2 \sigma^2})$
$- \int p(x) \ln p(x) dx = - \{$ 1️⃣ $\int \frac  {1}{(2 \pi  \sigma^2)^{1/2}}  \exp({-\frac{(x-\mu)^2}{2 \sigma^{2}}})(- \frac{1}{2}  \ln  {2\pi  \sigma^2})dx-$ 2️⃣ $\int \frac  {1}{(2 \pi  \sigma^2)^{1/2}}  \exp({-\frac{(x-\mu)^2}{2 \sigma^2}})(\frac{(x-\mu)^2}{2 \sigma^2})dx \}$
1️⃣ $\int \frac  {1}{(2 \pi  \sigma^2)^{1/2}}  \exp({-\frac{(x-\mu)^2}{2 \sigma^{2}}})(- \frac{1}{2}  \ln  {2\pi  \sigma^2})dx = - \frac{1}{2}  \ln 2 \pi  \sigma^2$
2️⃣ $\int \frac  {1}{(2 \pi  \sigma^2)^{1/2}}  \exp({-\frac{(x-\mu)^2}{2 \sigma^2}})(\frac{(x-\mu)^2}{2 \sigma^2})dx = - \frac {1}{2 \sigma^2} \int(x-\mu)^2\frac {1}{(2 \pi  \sigma^2)^{1/2}}  \exp({-\frac{(x-\mu)^2}{2 \sigma^{2}}})dx = - \frac {1}{2}$


이때 $E[(x-\mu)^2] = \sigma^2$는 variance이다. 따라서 $- \int p(x) \ln p(x) dx = \frac  {1}{2}  \ln  {2 \pi  \sigma^2} + \frac{1}{2}$가 된다.
$\sigma^2$이 커질수록(broader, uniform에 가까워질수록), entropy는 더욱 커진다. entropy는 음수도 될 수 있다. $(\sigma^2 < \frac  {1}{2 \pi e})$

그렇다면 평균 $\mu$와 분산 $\sigma^2$이 정해져 있고, 세 가지 제약 조건이 있을 때 differential entropy를 maximize 해보자.
제약 조건 세 가지는 다음과 같다.
- $\int_{\infin}^{-\infin} p(x) dx = 1$
- $\int_{\infin}^{-\infin} xp(x) dx = \mu$
- $\int_{\infin}^{-\infin} (x-\mu)^2p(x) dx = \sigma^2$

$H(x) = - \int p(x) \ln p(x) dx$
Lagrange multipliers와 calculus of variations를 이용하면
$p(x) = \exp(-1+ \lambda_1 + \lambda_2 x + \lambda_3 (x-\mu)^2)$
$p(x) = \frac{1}{(2 \pi  \sigma^2)^{1/2}}  \exp (-\frac{(x-\mu)^2}{2\sigma^2})$

따라서 entropy는 $\frac  {1}{2}  \ln  {2 \pi  \sigma^2} + \frac{1}{2}$가 된다. 이 값은 음수도 될 수 있다.

continuous r.v.의 평균과 분산이 정해져 있을 때 entropy는 Gaussian distribution를 따를 때 최대이다.
<br>

# 4. Entropy는 어디에 쓰일까?
 ## MNIST Dataset
 -   handwritten digits
 - Training set: 60,000 examples
-   Test set: 10,000 examples
-   \# class: 10 → 0, 1, 2, 3, 4, 5, 6, 7, 8, 9
<center><img src=https://github.com/sml09181/sml09181.github.io/assets/105408672/afad4117-e5e6-439b-9206-a235af5e94b8 width="60%" height="60%">

매우 쉬운 dataset이다.
 -   Training Accuracy: 100%
 -   Test Accuracy: 99% ⬆️

따라서 우리가 새로운 모델을 개발할 때, MNIST dataset에서 실패하면 다른 dataset에서도 실패하게 된다. 만약 MNIST에서 성공해도 다른 dataset에서의 성공 여부는 장담할 수 없다.
<br>
<br>
<center><img src=https://github.com/sml09181/sml09181.github.io/assets/105408672/b4b8837a-2cd7-4a11-b395-1d9a33205148 width="95%">

Data가 Neural Network(사진 속 회색 판)을 통하면 meaningful representation이 된다. 이때 Classifier는 output으로 각 class의 probability를 뱉는다.
   -   The probability of class 0: <span style="color:blueviolet">0.7</span>
   -   …
   -   The probability of class 9: <span style="color:blue">0.1</span>
   - 
-   Model **confidence** can be measured by (확신 정보니까 uniform하지 X)
    -   $\max_ip_i$ for $i = 0, ..., 9$
    -   **Negative Entropy**
<br>

# 5. Conditional Entropy
-   Conditional Entropy도 정의할 수 있다.
    -   즉, $x$에 대한 것은 이미 알고 있을 때, y에 대한 entropy
        
        -   정보량: $-\ln p(y|x)$
        -   그렇다면, 평균적인 정보량을 나타내는 entropy $H(y|x)$의 경우는 어떻게 될까?
    -   $H(y|x) = - \int \int p(y, x) \ln p(y|x) dydx$
        
        -   NOTE) $H(x) = - \int p(x) \ln p(x) dx$
        -   Conditional entropy of $y$ given $x$
    -   $H(x, y) = H(y|x) + H(x)$
        -   $H(x, y)$: entropy of $p(x, y)$
        -   $H(x)$: entropy of $p(x)$
        -   즉, $x$와 $y$를 표현하기 위해 필요한 정보량 = $x$를 표현하고, $y$ given $x$를 표현하는 정보량

# 6. KL Divergence
Kullback-Leibler divergence은 상대 엔트로피(relative entropy), 정보 획득량(information gain), information divergence와 같은 말이다.

-   지금까지 정보량과 엔트로피에 대해 살펴보았다.
    -   우리는 unknown distribution $p(x)$에 관심이 많다.
    -   하지만, 모르기에 $q(x)$로 대신 모델링하고자 한다.
    -   그때, 평균적으로 더 필요한 정보량은 어떻게 될까?
    -   $KL(p||q) = - \int p(x) \ln q(x) dx - (- \int p(x) \ln p(x) dx) \\\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, = - \int p(x) \ln (\frac {q(x)}{p(x)}) dx$ (0 이상)
-   KL divergence
    -   등호가 성립할 조건: $q(x)$와 $p(x)$가 같을 때
    -   $KL(p||q) \neq KL(q||p)$
    -   KL divergence > 0 → why?
    - 
## Convex
x값을 interpolation / y값을 interpolation

expectation이 부등호와 함께 나갔다 들어왔다 해도 된다.
<br>
<br>
<br>
<br>

Further:

 - [ ] 정보량의 rescale에 대해 다시 찾아보기
 - [ ] 결합 확률, 조건부 확률 등 정리하기
 - [ ] entropy의 종류 알아보기 -> 섀먼 엔트로피를 그냥 entropy로 부르는 것인지

<br>
Reference:<br>
- [정보량과 엔트로피의 의미](https://bskyvision.com/entry/%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0-%EC%A0%95%EB%B3%B4%EB%9F%89%EA%B3%BC-%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC%EC%9D%98-%EC%9D%98%EB%AF%B8) <br>
[정보이론: 엔트로피, KL-Divergence](https://reniew.github.io/17/) <br>
[R에서 섀넌 엔트로피(Shannon entropy) 구하기](https://m.blog.naver.com/pmw9440/221990235236) <br>
[Entropy란(평균정보량, 정보량의 기댓값)](https://dsaint31.tistory.com/entry/Math-Entropy-%EB%9E%80-%ED%8F%89%EA%B7%A0%EC%A0%95%EB%B3%B4%EB%9F%89-%EC%A0%95%EB%B3%B4%EB%9F%89%EC%9D%98-%EA%B8%B0%EB%8C%93%EA%B0%92) <br>
[Shannon Entropy](https://enfow.github.io/study/statistics/2020/02/06/shannon_entropy/) <br>
- [라그랑주 승수법 예제🌟](https://subprofessor.tistory.com/65) <br>
- [초보를 위한 정보이론 안내서 - 1. Entropy란 무엇일까🌟](https://hyunw.kim/blog/2017/10/14/Entropy.html)
-[엔트로피와 크로스엔트로피](https://westshine-data-analysis.tistory.com/83#:~:text=%E2%96%B6%ED%81%AC%EB%A1%9C%EC%8A%A4%20%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC%EB%9E%80&text=%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC%EA%B0%80%20%EC%A0%95%EB%8B%B5%EC%9D%B4%20%EB%82%98%EC%98%AC,%EC%9E%88%EB%8A%94%20%EC%A0%95%EB%B3%B4%EB%9F%89%EC%9D%84%20%EC%9D%98%EB%AF%B8%ED%95%9C%EB%8B%A4.)

<kbd>hello</kbd>

<a>hello</a>

<code>hello</code>

<mark>hello</mark>

<under>hello</under>