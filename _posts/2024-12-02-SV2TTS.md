---
title: SV2TTS
author: Su
date: 2024-12-02 11:00:00 +0800
categories: [Paper Review]
tags: [Audio]
pin: false
use_math: true
---

[Paper Link](https://arxiv.org/pdf/1806.04558) [Github Link](https://github.com/CorentinJ/Real-Time-Voice-Cloning)

## 🍊 Key Takeaways
- 1️⃣ Speaker Encoder의 학습 목표를 Speaker Verification으로 설정하였다. 이로도 충분한 speaker embedding 학습이 가능하다.  
- 2️⃣ Speaker Encoder와 Synthesizer의 훈련을 분리하여 학습에 필요한 labeled Multi-speaker dataset의 크기를 줄였다. 
- 3️⃣ Voice Cloning: Unseen speaker zero-shot inference 가능


# 1. Overview

![image](https://github.com/user-attachments/assets/56909b54-987d-4129-95d7-f92f4ff35174)

아래 세 가지 요소로 이루어져 있다. 각 요소는 독립적으로 훈련된다.
- 1️⃣ **speaker encoder NW**
	- task: speaker verification
- 2️⃣ **seq2seq synthesis NW**
	- based on Tacotron2
	- speaker embedding → mel-spectrogram
- 3️⃣ **auto-regressive vocoder NW**
	- based on WaveNet
	- mel-spectrogram → time-domain waveform

### Overall goal

- Speaker Verification으로 학습한 speaker embedding을 이용하여 transfer learning으로 Multispeaker TTS 수행
- Unseen speaker도 zero-shot으로 생성 가능 


# 2. Multispeaker speech synthesis model

## 2.1 Speaker encoder
- Input: log mel-spectrogram (from raw speech data w/ arbitrary length)
- Output: speaker embedding w/ fixed dimension 
- Action: Learn speaker embedding via speaker verification task
	- d-vector 사용 
	- 같은 화자의 임베딩은 가깝게, 서로 다른 화자의 임베딩은 서로 멀게 (cosine similarity)

Speaker encoder는 이후에 나오는 synthesis NW를 원하는 target speaker의 참조 음성을 이용해 condition하는 데에 사용된다. 이후에 Zero-shot inference를 해야 하기 때문에 짧은 speech만으로도 그 화자를 식별할 수 있는 능력을 갖춰야 한다. 

dataset은 speech data와 speaker label로만 이루어지고, text 정보는 포함되지 않는다. 

40-channel log-mel spectrogram이 input이다. Speaker encoder NW은 768 cells로 구성된 3개의 LSTM layers로 이루어져 있다. 이때  각 레이어 뒤에는 256-dimensional 크기로 projection된다다. 최종 embedding은 마지막 frame에서 top layer의 출력을 L2 normalization하여 생성된다. 추론 시에는 임의 길이의 utterance를 800ms windows로 나누어 50%씩 overlap하여 처리한다. 네트워크는 각 window에 독립적으로 실행되며, output은 averaging되고 normalization되어 최종 utterance embedding을 생성한다.

화자의 특징을 직접 학습시키지는 않지만, speaker verification task만으로도 충분한 화자 임베딩이 학습된다. 

## 2.2 Synthesizer
- Input: grapheme or phoneme sequence
- Output: log-mel spectrogram
- Action: target speaker에 대한 Text-to-spectrogram

SV2TTS의 synthesizer는 한 명의 화자만 지원 가능했던 Tacotron2를 확장하여 multi-speaker에도 적용할 수 있게 만들었다. 따라서 Tacotron2에 대해 간단하게 정리하고 살펴 보자. 

![image](https://github.com/user-attachments/assets/ea809893-2c6d-42c3-aef2-4267aecc6475)

Tacotron 2의 overview 그림이다. Tacotron2는 크게 두 가지 stage로 이루어져 있다. 
- 1️⃣ **Tacotron 2(Seq2Seq w/ attention)**: text → Mel-spectrogram
	- Encoder: character(one-hot) → fixed hidden feature
	- Location Sensitive Attention: Decoder에 제공할 정보 alignment 
	- Decoder: alignment feature, 이전 time step 에서 생성된 mel-spectrogram → 다음 time step의 mel-spectrogram
		- terminal state 확률도 함께 계산 
- 2️⃣ **WaveNet의 변형**: Mel-spectrogram → Wavefrom 


SV2TTS에서는 DeepVoice2과 유사하게 target speaker에 대한 embedding vector를 각 time step에서 synthesizer encoder output과 concatenate하였다. 하지만 DeepVoice2와 달리 embedding을 attention layer에 전달하는 방식만으로도 다양한 speakers에 대해 converge하는 데에 충분하다. Synthesizer의 train data에는 Speaker Identity Label이 필요 없다. 

또한 Loss는 spectrogram에 대한 L2 loss에 추가로 L1 loss를 더한 형태이다. L1 loss를 더한 것은 noisy한 train dataset에서의 robustness를 올려 준다. embedding에 기반한 additional한 loss term은 따로 없다. 즉 Triplet loss나 Contrastive loss 없이 훈련이 가능하다. 


## 2.3 Neural vocoder
- Input: log-mel spectrogram
- Output: time-domain waveform

Tacotron2에서 확장한 WaveNet과 동일하다. 30개의 dilated convolution layers로 구성된다. vocoder NW는 encoded vector에 직접적으로 condition되지 않는다.


## 2.4 Inference and zero-shot speaker adaptation

![image](https://github.com/user-attachments/assets/e84e65b3-1231-40f3-8931-66b208e4fdb0)

Inference시 reference speech는 arbitrary untranscribed speech audio이다. TTS하고 싶은 text와 일치할 필요가 없다. 이때 unseen speaker도 짧은 speech만으로 생성해낼 수 있다(Zero-shot). 위 그림을 보면 reference speech의 spectrogram과 합성된 spectrogram이 거의 흡사하다고 볼 수 있다. 


# 3. Experiments

## Dataset
- For Tacotron2, WaveNet: VCTK(109명 화자, 각 44시간)
- For speaker encoder: 1.8만 명 화자, 평균 3.9초 길이의 3600만 개 문장

## Speaker Embedding Space
이 논문을 처음 보았을 때 가장 흥미로웠던 부분이다. 

![image](https://github.com/user-attachments/assets/21f11336-d033-4012-aa2c-9cd2458a9a32)

PCA와 t-SNE 시각화에서 화자들은 성별에 따라 잘 구분되는 모습을 보인다. 모든 여성 speaker의 sample은 왼쪽에, 모든 남성 speaker의 sample은 오른쪽에 위치한다. 이는 화자 인코더가 화자 공간에 대한 합리적인 표현을 학습했다는 것을 나타낸다.

## Reference

- [SV2TTS - Speaker verification을 Multi-Speaker에 활용한 TTS 모델 (Voice Cloning)](https://velog.io/@hws0120/SV2TTS-Speaker-verification%EC%9D%84-Multi-Speaker%EC%97%90-%ED%99%9C%EC%9A%A9%ED%95%9C-TTS-%EB%AA%A8%EB%8D%B8-Voice-Cloning)
- [Tacotron2 도식도](https://joungheekim.github.io/2020/10/08/paper-review/)
