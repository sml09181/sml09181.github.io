---
title: Deep Learning Specialization 4-2 | Convolutional Neural Networks
author: Su
date: 2023-12-30 03:55:00 +0800
categories: [DL]
tags: [EURON]
pin: false
use_math: true
---

Learning Source
+ [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning?utm_source=deeplearningai&utm_medium=institutions&utm_campaign=SocialYoutubeDLSC1W1L1#courses)
+ [부스트코스 딥러닝 4단계: 합성곱 신경망 네트워크(CNN)](https://www.boostcourse.org/ai218/lecture/34895)

<br>

## **Why Look at Case Studies?**
+ 효율적인 신경망 구조 예시를 살펴 보며 합성곱 신경망을 구축하자
  + 하나의 컴퓨터 비전 작업에서 잘 작동한 구조가 다른 작업에도 유용하고 잘 작동하기 때문이다.
+ 대표적인 신경망 예시
  + LeNet - 5
  + AlexNet
  + VGG
  + ResNet
  + Inception

## **Classic Networks**

### LeNet-5
<br>
<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/d4c7614d-eecb-46dd-ab72-4accaf72a62a">

+ 목적: 흑백으로 된 손글씨 인식
+ 최근에 비해 상대적으로 적은 변수 개수
+ 다만, 해당 논문에서는 최근과 달리 pooling layer 뒤에 비선형함수 적용. 
  + 비선형함수도 ReLU가 아닌 Sigmoid 적용
  + 더 자세한 내용 ->  논문 섹션 2, 3

#### 정리

+ 당시에는 padding을 사용하지 않고 Valid convoluation을 사용했다.
  + 따라서 합성곱 층을 적용할 때마다 높이와 너비가 감소함
  + ReLU가 사용되기 전이었다.
  + 비선현성이 pooling 뒤에 있었다.
+ 요즘도 $n_H$ 와 $n_W$ 는 감소, $n_C$ 는 증가하는 경향을 보인다.
  + conv - pool - conv - pool - fc - fc - output<br>

### AlexNet
<br>
<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/664e5304-486f-4a06-b1cd-3c607ba5d38b">

+ 목적: 이미지를 1000개 클래스로 분류
+ LeNet에 비해서 굉장히 많은 변수
+ 활성화 함수로 ReLU 사용
+ 2개의 GPU 사용
+ “합성곱을 같게 가져간다(same)”: 이전 층의 높이와 넓이를 같게 만드는 padding을 가진다<br>

### VGG-16
<br>
<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/60fbb3c6-0966-400a-9a82-3d18bc077e03">

+ in 모든 합성곱 연산
  + 3 x 3 의 필터
  + padding size: 2
  + 스트라이드는 1로 하고, 
  + 2 x 2 픽셀씩 Max Pooling
+ AlexNet에 비해 간결한 구조
+ 산출값의 높이와 넓이는 매 max pooling을 거칠 때마다 1/2씩 줄어들며, channel의 수는 두 배 혹은 세 배로 늘어난다.
  + 균일하다
+ 단점: 훈련시킬 변수의 개수가 많아 네트워크의 크기가 커진다


## **Residual Networks(ResNets)**
+ 아주 깊은 신경망을 학습하지 못하는 이유 중 하나: 경사가 소실되거나 폭발적으로 증가하기 때문
  + 하지만 ResNet에서는 스킵 연결로 해결
+ 잔여 블록을 설명하기 전에 아래의 두 층의 신경망이 있다고 가정
+ 아래 그림처럼 모든 층을 지나는 연산 과정을 “main path” 라고 부른다.
  + 즉, $a^{[l]}$ 의 정보가  $a^{[l+2]}$ 로 흐르기 위해서는 모든 과정을 거쳐야 한다.<br>

<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/cefa7393-89e4-4aa3-8bb9-82a7234c8719">

+ 하지만 ResNet에서는 아래 그림 처럼 $z^{[l+2]}$ 에 비선형성을 적용해주기 전에 $a^{[l]}$ 을 더하고 여기에 다시 비선형성을 적용한다.<br>

<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/28508190-dd8e-4900-a4cc-363cf98eb5c0">

+ Residual Block(잔여 블록): $a^{[l]}$ 를 더해서 다시 활성화 함수에 넣는 부분까지
  + Short cut(=Skip Connection): $a^{[l]}$ 의 정보를 더 깊은 층으로 전달하기 위해 일부 층을 뛰어 넘는 역할
+ ResNet 은 여러 개의 잔여 블록으로 구성
  + 평형망에 스킵 연결을 더해주면 된다.<br>

<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/ea3ed68a-8a0a-4c3d-9d72-e5ed6ab64cc1">

+ 또한, 경험적으로 층의 개수를 늘릴수록 훈련 오류는 감소하다가 다시 증가한다.
  + 하지만 이론 상으로는 신경망이 깊어질수록 훈련 세트에서 오류는 계속 낮아져야 한다.
  + 하지만 ResNet 에서는 훈련 오류가 계속 감소하게 할 수 있다.<br>

<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/c79fa6c5-c7ea-492e-9d0f-7b6de4f6c7a7">

## **Why ResNets work**
+ 신경망의 깊이가 깊어질수록 훈련세트를 다루는 데에 지장이 있을 수 있다. 
  + ResNet 은 이를 잘 해결했다.<br>

<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/624798a8-f751-44f9-8b61-841fdc8009fb">

+ 위와 같은 큰 신경망에서 두 개의 층을 더 추가하고 지름길을 연결해준다.
  + 활성화함수: ReLU
+ 스킵 연결을 더해준 출력값: $a^{[l+2]}$ 은 $g(z^{[l+2]} + a^{[l]}) = g(W^{[l+2]}a^{[l+1]} + b^{[l+2]}+ a^{[l]})$
  + 여기서 만약에 $W^{[l+2]}$ 와 $b^{[l+2]}$ 의 값이 0이 된다면, 위의 식은 $a^{[l+2]} = g(a^{[l]}) = a^{[l]}$ 으로 항등식이 된다.
+ 위 항등식의 의미: 신경망으로 하여금 스킵 연결을 통해 두 층이 없는 더 간단한 항등식을 학습하여, 두 층 없이도 더 좋은 성능을 낼 수 있게 만든다는 것
+ 다만, $z^{[l+2]}$ 와 $a^{[l]}$ 이 같은 차원을 가져야 한다. 따라서 보통 동일합성곱 연산(출력 크기가 입력 크기와 같게 하는 합성곱연산)을 하거나 차원을 같게 만들어주는 행렬 $W_s$ 를 잔여블록 앞에 곱해줘서 같게 만든다.
  + 학습되는 행렬, 0 행렬 모두 가능
+ ResNet이 잘 작동하는 이유
  + 추가된 층이 항등 함수를 학습하기에 용이하기 때문
  + 따라서 성능의 저하가 없다는 것을 보장할 수 있다.


## **Network in Network and 1x1 convolutions**
+ 합성곱 신경망을 구축할 때 1 x 1 합성곱은 매우 유용하다.

<br>
<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/d03c722a-3914-4c8f-96d4-d6a599a332e4">

+ 위의 예시처럼, 195 개의 입력숫자가 32개의 1 x 1 필터와 합성곱을 하여 32 개의 출력 숫자가 된다. 
  + 즉, 이는 입력 채널의 수만큼 유닛을 입력으로 받아서, 이들을 하나로 묶는 연산과정을 통해, 출력채널의 수만큼 출력을 하는 작은 신경망 네트워크로 간주 할 수 있다. 
  + 따라서 네트워크 안의 네트워크라고도 한다.
+ 이처럼 1x1 합성곱 연산을 통해 비선형성을 하나 더 추가해 복합한 함수를 학습 시킬 수 있고, 채널수를 조절 해줄 수 있다.
+ 채널이 곱해질수록 1x1 convolution이 더 의미있다.
+ FC를 각 위치에 각각 적용해서 filter의 수만큼 출력하는 것
+ $n_C$ 를 줄일 수 있다. 

## **Inception Network Motivation**

+ 필터의 크기나 풀링을 결정하는 대신 전부 다 적용해서 출력들을 합친 뒤 네트워크로 하여금 스스로 변수나 필터 크기의 조합을 학습하게 만드는 것


<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/2ccdecfd-20ad-40a5-a927-cfb2de032599">

+ 위와 같은 인셉션 네트워크의 문제: 계산 비용입
+ 단순 5 x 5 필터여도 필요한 곱셈: 28 x 28 x 32 x 5 x 5 x 192 = 약 1억 2000 만개
  + 이를 1 x 1 합성 곱으로 해결할 수 있다.

<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/a8b87bed-4165-41a4-834f-3c5ea1e20741">

+ 5 x 5 합성곱을 사용하기 전에 1 x 1 의 합성곱 연산을 통해 입력 이미지의 볼륨을 줄이는 작업을 한다. 그 후에 다시 5 x 5 합성곱 연산을 하는데, 이때 계산 비용은 약 1240 만개로 아래와 같다.
  + 1 x 1 합성곱: 28 x 28 x 16 x 1 x 1 x 192 =  약 240 만개
  + 5 x 5 합성곱: 28 x 28 x 32 x 5 x 5 x 16 = 약 1000 만개
+ 학습에 필요한 계산 비용이 1/10 수준으로 크게 줄어든 것을 알 수 있습니다. 여기서 사용된 1 x 1 합성곱 층을 “병목 층”이라고도 한다.
+ 병목층을 사용시 표현의 크기가 줄어들어 성능에 영향을 지장을 줄지 걱정 될 수도 있는데, 적절하게 구현시 표현의 크기를 줄임과 동시에 성능에 큰 지장 없이 많은 수의 계산을 줄일 수 있다.
+ bottleneck layer: Network에서 가장 작은 부분
  + 크기를 다시 늘이기 전 이미지를 줄이는 것


## **Inception Network**
<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/f3405763-a259-412d-a0c4-a52370739de5">

+ 인셉션 네트워크는 위 그림과 같이 여러개의 인셉션 모듈로 구성 되어 있다.

+ 전체 모델 구조

<br>
<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/875dc637-c4f0-421d-98c1-9fb23b99878c">
  
+ 중간 중간에 차원을 바꾸기 위한 최대 풀링층을 포함해서 여러 개의 인셉션 블록이 계속 반복된다.
+ 인셉션 네트워크는 “구글넷”이라고도 한다.

<br>
<br>