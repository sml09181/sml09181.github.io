---
title: 자연어 처리의 모든 것 | 2. 자연어 처리와 딥러닝
author: Su
date: 2024-01-12 01:11:00 +0800
categories: [DL]
tags: [EURON]
pin: false
use_math: true
---

Learning Source
+ [자연어 처리의 모든 것](https://www.boostcourse.org/ai330/lecture/1455360)


<br>

## **Recurrent Neural Network(RNN)**

### RNN(Recurrent Neural Network)

<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/af613aaf-8e9d-4732-87b9-c28fb414357e">


+ <code>RNN</code>: 현재 타임스텝에 대해 이전 스텝까지의 정보를 기반으로 예측값을 산출하는 구조의 딥러닝 모델
+ 매 타임스텝마다 동일한 파라미터를 가진 모듈을 사용하므로 '재귀적인 호출'의 특성을 보여주어 'Recurrent Neural Network'라는 이름을 가지게 되었다.

### RNN 계산 방법

<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/e9096ea9-7e93-4fe5-8100-4f1d36204fea">

<br>
<img width="267" alt="image" src="https://github.com/sml09181/sml09181.github.io/assets/105408672/7c0bbcd0-1554-4255-b476-ac2bdb087dce">

+ NOTICE: the same function and the same set of parameters are used at every time step.
+ 1️⃣ 위의 변수들에 대하여, $h_t = f_w(h_{t-1}, x_t)$ 의 함수를 통해 매 time step마다 hidden state를 다시 구해준다.
+ 2️⃣ 이때, $W$ 와 입력값 ($x_t, h_{t-1}$) 으로 $\tanh$ 를 곱해서 $h_t$ 를 구해 준다.
+ 3️⃣ 구해진 $h_t, x_t$ 를 입력으로 $y_t$ 값을 산출하게 된다. <br>

<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/7a30af0c-1d1d-47bf-baf9-206c9d58cb51">



### 다양한 타입의 RNN 모델
<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/f2fdc1d2-d57f-450b-926f-41334798366a">

+ one to one
+ one to many: image captioning(image ➡️ sequence of words)
+ many to one: action prediction(sequence of video frames ➡️ action class)
+ many to many
    + many to one + one to many
    + type 1: video captioning(sequence of video frames ➡️ caption)
    + type 2: video classification on frame level



## **Character-level Language Model**

<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/3839236d-d8d9-4ed0-8266-f5606f6164bb">

+ 언어 모델: 이전에 등장한 문자열을 기반으로 다음 단어를 예측하는 task
  + 그중에서도 character-level Language Model은 문자 단위로 다음에 올 문자를 예측하는 언어 모델이다.
+ 예를 들어, 그림과 같이 맨 처음에 "h"가 주어지면 "e"를 예측하고, "e"가 주어지면 "l"을 예측하고, "l"이 주어지면 다음 "l"을 예측하도록 hidden state가 학습돼야 한다.
+ 이때 각 타임스텝별로 output layer를 통해 차원이 4(유니크한 문자의 개수) 벡터를 출력해주는데 이를 logit이라고 부르며, softmax layer를 통과시키면 원-핫 벡터 형태의 출력값이 나오게 된다.
  + `Logits`: unnormalised(or not-yet normalised) predictions(or outputs) of a model. These can give results, but we don't normally stop with logits, because interpreting their raw values is not easy. 


## **Backpropagation through time and Long-Term-dependency**

### RNN 모델이 학습하는 방법: Truncation, BPTT
+ <code>Truncation</code>: 제한된 리소스(메모리) 내에서 모든 시퀀스를 학습할 수 없기때문에 아래 사진과 같이 잘라서 학습에 사용하는 것
+ Carry hidden states forward in time forever, but only backpropagate for some smaller number of steps


## **Long Short-Term Memory(LSTM)**
+ Idea: 단기 기억으로 저장하여 이걸 때에 따라 꺼내 사용함으로 더 오래 기억할 수 있도록 개선하는 것
  + 다시 말해 Cell state에는 핵심 정보들을 모두 담아두고, 필요할 때마다 Hidden state를 가공해 time step에 필요한 정보만 노출하는 형태로 정보가 전파됨<br>
  
<img width="356" alt="image" src="https://github.com/sml09181/sml09181.github.io/assets/105408672/77572459-4101-4cca-9cd4-4b60b94184b8">

### LSTM이 RNN과 다른점
+ LSTM: 각 time step마다 필요한 정보를 단기 기억으로 hidden state에 저장하여 관리되도록 학습
+ backpropagation 진행시 가중치(W)를 계속해서 곱해주는 연산이 아니라, forget gate를 거친 값에 대해 필요로하는 정보를 덧셈을 통해 연산하여 그레디언트 소실/증폭 문제 방지

### GRU: Gated Recurrent Unit

<img width="372" alt="image" src="https://github.com/sml09181/sml09181.github.io/assets/105408672/6497a7c9-e38a-4601-8dd2-b3efc4d49087">

### RNN, LSTM, GRU 요약
+ RNN: 들어오는 입력값에 대해서, 많은 유연성을 가지고 학습되는 딥러닝 모델
  + RNN에서는 그레디언트 소실/증폭 문제가 있어 실제로 많이 사용되지는 않지만, RNN 계열의 LSTM, GRU 모델은 현재도 많이 사용되고 있다.
+ LSTM과 GRU 모델은 RNN과 달리 가중치를 곱셈이 아닌 덧셈을 통한 그레디언트 복사로 그레디언트 소실/증폭 문제 해결 
  + 덧셈: back propagation을 수행할 때 gradient를 복사해주는 효과를 준다. 

<br>
<br>
Source<br>

+ [Unrolled RNN](https://necst.it/exploring-boundary-accuracy-performances-recurrent-neural-networks/rnn-unrolled/)
+ [RNN](https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/)
+ [Character-Level Language Model](https://towardsdatascience.com/character-level-language-model-1439f5dd87fe)
+ [Logits](https://datascience.stackexchange.com/questions/31041/what-does-logits-in-machine-learning-mean)
<br>