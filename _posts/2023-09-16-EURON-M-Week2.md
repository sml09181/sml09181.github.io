---
title: Deep Learning Specialization 1-3 | Basics of Neural Network Programming
author: Su
date: 2023-09-16 05:55:00 +0800
categories: [DL]
tags: [EURON]
pin: false
use_math: true

---

Learning Source
+ [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning?utm_source=deeplearningai&utm_medium=institutions&utm_campaign=SocialYoutubeDLSC1W1L1#courses)
+ [ë¶€ìŠ¤íŠ¸ì½”ìŠ¤ ë”¥ëŸ¬ë‹ 1ë‹¨ê³„: ì‹ ê²½ë§ê³¼ ë”¥ëŸ¬ë‹](https://m.boostcourse.org/ai215/lectures/86249)

# **íŒŒì´ì¬ê³¼ ë²¡í„°í™”**
## Vectorization
### What is vectorization?
+ $z=w^Tx +b$ ì„ êµ¬í•´ë³´ì.<Br>
  $w= \begin{bmatrix}
      w_1 \\
      w_2 \\
      ... \\
      w_{n_x} \\ \end{bmatrix}$ (ì—´ë²¡í„°) 
  <br>

  $x= \begin{bmatrix}
      x_1 \\
      x_2 \\
      ... \\
      x_{n_x} \\  \end{bmatrix}$ (ì—´ë²¡í„°)
+ ì´ë•Œ $w \in \mathbb{R}^{n_x}, x \in \mathbb{R}^{n_x}$
+ 1ï¸âƒ£ **Non-vetorized**
  ```python
    z=0
    for i in range(n_x):
      z+= w[i]*x[i]
    z+=b
  ```
+ 2ï¸âƒ£ **Vectorized**
  ```python
    z = np.dot(w, x) + b
  ```
+ <code>SIMD</code>(Single Instruction Multiple Data)
  + ë³‘ë ¬ í”„ë¡œì„¸ì„œì˜ í•œ ì¢…ë¥˜ë¡œ, í•˜ë‚˜ì˜ ëª…ë ¹ì–´ë¡œ ì—¬ëŸ¬ ê°œì˜ ê°’ì„ ë™ì‹œì— ê³„ì‚°í•˜ëŠ” ë°©ì‹
  + ë²¡í„°í™” ì—°ì‚°ì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.
  + forë¬¸ìœ¼ë¡œ í•˜ë‚˜ì˜ ê°’ì„ ì—°ì‚°í•˜ëŠ” ê²ƒë³´ë‹¤ vectorizedí•˜ì—¬ í•œ ë²ˆì— ì—°ì‚°í•˜ëŠ” ê²ƒì´ ë” ë¹ ë¥´ê³  íš¨ìœ¨ì ì´ë‹¤.
  ```python
    import numpy as np
    a = np.array([1, 2, 3, 4])
    print(a) # [1, 2, 3, 4]
  ```
  ```python
    import time
    a = np.random.rand(100000)
    b = np.random.rand(100000)

    tic = time.time()
    c = np.dot(a, b)
    toc = time.time()

    print(c)
    print("Vectorized version: " + str(1000*(toc-tic)) + "ms")
    # 24960.839226052667
    # Vectorized version: 1.783132553100586ms

    c=0
    tic = time.time()
    for i in range(100000):
      c += a[i]*b[i]
    toc = time.time()

    print(c)
    print("For loop:" + str(1000*(toc-tic))+"ms")
    # 24960.839226052893
    # For loop:145.95532417297363ms
  ```

## More Vectorization Examples
+ Whenever possible, avoid explicit for-loops
  + 1ï¸âƒ£ **Non-vetorized**
    + $ u = Av$
    + $u_i = \sum_i \sum_j A_{ij}v_{ij}$
    + $u = \operatorname{np.zeros}((n, 1))$
  ```python
  u = np.zeros((n, 1))
  for i ...
    for j ...
      u[i] += A[i][j] * v[j]
  ```
  + 2ï¸âƒ£ **Vectorized**
    ```python
      u = np.dot(A, v)
    ```
+ Vectors and matrix valued functions
  + Say you need to apply the exponential operation on every element of a matrix/vector. <br>

  $ v = \begin{bmatrix} v_1 \\ ... \\ v_n \\ \end{bmatrix} \to u = \begin{bmatrix} e^{v_1} \\ ... \\ e^{v_n} \\ \end{bmatrix}$ (ë‘˜ë‹¤ ì—´ë²¡í„°)  
  

  ```python
      u = np.zeros((n, 1))
      for i in range(n):
        u[i]=math.exp(v[i])
    ```
    ```python
    import numpy as np
    u = np.exp(v)
    u = np.log(v)
    u = np.abs(v)
    np.maximum(v, k)

    np.maximum(np.eye(2), [0.5, 2]) # broadcasting
    '''
    array([[ 1. ,  2. ],
        [ 0.5,  2. ]])
    '''
  ```

+ Logistic regression derivatives
  + $dw$ ë¥¼ vectorized ì‹œí‚´ìœ¼ë¡œì¨ ê°€ì¥ ì•ˆìª½ì˜ forë¬¸ì€ ì œê±°í–ˆì§€ë§Œ, ì—¬ì „íˆ ê°€ì¥ ë°”ê¹¥ì˜ forë¬¸ì´ ë‚¨ì•„ ìˆë‹¤.<br>
  <img width="550" alt="image" src="https://github.com/sml09181/sml09181.github.io/assets/105408672/16ab9414-2968-4530-9798-d912bc3f3c02">

  + ìì£¼ ì“°ëŠ” ë„˜íŒŒì´(numpy) í•¨ìˆ˜:
    + `log`
    + `abs`
    + `maximum`
    + `**`
    + `zeros`


## Vectorizing Logistic Regression
+ ì•„ë˜ì˜ ì‹ì€ forë¬¸ì„ ì´ìš©í•´ iì˜ ê°’ì„ ë³€í™”ì‹œí‚¤ë©° ê³„ì‚°í•´ì•¼ í•œë‹¤.
  + $z^{(i)} = W^Tx^{(i)}+b$
  + $a^{(i)} = \sigma(z^{(i)})$
+ í•˜ì§€ë§Œ ê³„ì‚°ì˜ íš¨ìœ¨ì„±ì„ ì¦ê°€ì‹œí‚¤ê¸° ìœ„í•´ ë²¡í„°ë¥¼ ì´ìš©í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  + `Z = np.dot(np.transpose(W), X) + b`
+ ìœ„ì˜ ì½”ë“œì—ì„œ $(1,m)$ í¬ê¸°ì˜ matrixì™€ ìƒìˆ˜ $b$ ë¥¼ ë”í•˜ê¸°ì— ì˜¤ë¥˜ê°€ ë‚  ê²ƒ ê°™ì§€ë§Œ, íŒŒì´ì¬ì´ ìë™ì ìœ¼ë¡œ ìƒìˆ˜ë¥¼ $(1,m)$ í¬ê¸°ì˜ í–‰ë ¬ë¡œ broadcasting í•´ì£¼ê¸° ë•Œë¬¸ì— ë°œìƒí•˜ì§€ ì•ŠëŠ”ë‹¤.
- $ X = \begin{bmatrix} \mid & \mid &  & \mid \\ X^{(1)} & X^{(2)} & ... & X^{(m)} \\ \mid & \mid &  & \mid \\ \end{bmatrix} $
+ $Z = [Z^{(1)}, Z^{(2)}, ..., Z^{(m)}]=w^TX+[b, b, ..., b] \\ = w^TX^{(1)}+b, w^TX^{(2)}+b, ..., w^TX^{(m)}+b$
+ ì¦‰, `Z = np.dot(W.T, X) + b`ì´ë‹¤(ì´ë•Œ $b$ëŠ” ì‹¤ìˆ˜)
+ $A=[a^{(1)}, a^{(2)}, ..., a^{(m)}] = \sigma(Z)$


## Vecotorizing Logistic Regression's Gradient Computation

### Vectorizing Logistic Regression
+ ì´ë¯¸ ê²‰ì˜ forë¬¸ì„ ì œê±°í•´ì„œ $dw$ ëŠ” ë²¡í„°ì´ë¯€ë¡œ $dw_1, dw_2, ..., dw_3$ ë¥¼ ê°ê° ê³„ì‚°í•˜ì§€ ì•ŠëŠ”ë‹¤.
<br>

  $A=[a^{(1)}, a^{(2)}, ..., a^{(m)}], Y = [y^{(1)}, y^{(2)}, ..., y^{(m)}]$ <BR>
  $dz =[dz^{(1)}, ..., dz^{(m)}] = A-Y $<BR>
  $ dz^{(1)} = a^{(1)} - y^{(1)}, ..., dz^{(m)} = a^{(m)} - y^{(m)} $

+ 1ï¸âƒ£ **Non-vetorized**
  + $dw$ êµ¬í•˜ê¸°<br>
    $ dw = 0$ <br>
    $dw += X^{(1)}dz^{(1)}$<br>
    $dw += X^{(2)}dz^{(2)}$<br>
    $...$<br>
    $dw += X^{(m)}dz^{(m)}$<br>
    $ dw /= m$
  + $db$ êµ¬í•˜ê¸°<br>
    $ db = 0$ <br>
    $db += dz^{(1)}$<br>
    $db += dz^{(2)}$<br>
    $...$<br>
    $db += dz^{(m)}$<br>
    $ db /= m$
+ 2ï¸âƒ£ **Vectorized**
  + $dw$ êµ¬í•˜ê¸°<br>
    $dw = \frac{1}{m} Xdz^T$
    $  =\frac{1}{m}$
  + $db$ êµ¬í•˜ê¸° <br>
    $db = \frac{1}{m} \sum_{i=1}^{m} z^{(i)}=\frac{1}{m} \operatorname{np.sum}(dz)$ <br>
    $=\frac{1}{m} \begin{bmatrix} \mid & \mid & 0 & \mid \\ X^{(1)} & X^{(2)} & ... & X^{(m)} \\ \mid & \mid & 0 & \mid \\  \end{bmatrix} \begin{bmatrix} dz^{(1)} \\  ... \\ dz^{(m)} \\  \end{bmatrix} $
+ $dz$ ëŠ” column vectorì´ë‹¤.

### Implementing Logistic Regression
+ 1ï¸âƒ£ **Non-vetorized** 
  $ J=0, dw_1=0, dw_2=0, db=0$<br>
  $ \operatorname{for} i = 1 \operatorname{to} m:
      \\ Z^{(i)} = w^Tx^{(i)} + b
      \\ a^{(i)} = \sigma(z^{(i)})
      \\ J += -[y^{(i)}\log a^{(i)} + (1-y^{(i)})\log (1-a^{(i)})]
      \\ dz^{(i)} = a^{(i)} - y^{(i)}
      \\ dw_1 += {x_1}^{(i)}dz^{(i)}
      \\ dw_2 += {x_2}^{(i)}dz^{(i)}
      \\ ...
      \\ dw_{n_x} += {x_{n_x}}^{(i)}dz^{(i)}
      \\ db += dz^{(i)}
    \\ J = J / m, dw_1 = dw_1 / m, dw_2 = dw_2 / m
    \\ db = db/m $
+ 2ï¸âƒ£ **Vectorized**
  for iter in range(1000):
  $ Z = w^TX + b
    \\  = \operatorname{np.dot}(w.T, X) + b
    \\ A = \sigma(Z)
    \\ dZ = A - Y
    \\ dw = \frac{1}{m}XdZ^T
    \\ db = \frac{1}{m} \operatorname{np.sum}(dz)
    \\ w := w - \alpha dw
    \\ b := b - \alpha db$

## Broadcasting in Python

### numpy.random ê´€ë ¨ ì •ë¦¬
+ <code>np.random.rand()</code>: 0~1 ì‚¬ì´ ê· ì¼ ë¶„í¬ ì¶”ì¶œ í•¨ìˆ˜
  + ê´„í˜¸ ì•ˆì„ ë¹„ì›Œë‘ë©´ ê°’ 1ê°œê°€ ì¶”ì¶œëœë‹¤
  + ê´„í˜¸ ë‚´ì— dimensionì„ ì ìœ¼ë©´, í•´ë‹¹ dimensionì˜ numpy arrayê°€ ìƒì„±ëœë‹¤
  ```python
    np.random.rand() # 0.6320642827185263
    np.random.rand(3, 2)
    '''
    array([[0.59950361, 0.81513226],
        [0.0775172 , 0.33192424],
        [0.2379349 , 0.84078282]])
    '''
  ```
+ <code>np.random.random()</code>: 0~1 ì‚¬ì´ ê· ì¼ ë¶„í¬ ì¶”ì¶œ í•¨ìˆ˜
  + rand í•¨ìˆ˜ì™€ ê±°ì˜ ë™ì¼í•˜ë‚˜, ì›í•˜ëŠ” ì°¨ì›ì˜ í˜•íƒœë¥¼ íŠœí”Œ ìë£Œí˜•ìœ¼ë¡œ ë„£ì–´ì£¼ì–´ì•¼ í•œë‹¤
  + ê´„í˜¸ ì•ˆì„ ë¹„ì›Œë‘ë©´ ê°’ 1ê°œê°€ ì¶”ì¶œëœë‹¤
  + ê´„í˜¸ ë‚´ì— dimensionì„ ì ìœ¼ë©´, í•´ë‹¹ dimensionì˜ numpy arrayê°€ ìƒì„±ëœë‹¤
  ```python
    np.random.random() # 0.7256115181737749
    np.random.random((3, 2))
    '''
    array([[0.21562737, 0.69082549],
        [0.81584948, 0.61564714],
        [0.85217779, 0.70106302]])
    '''
  ```
+ <code>np.random.randn</code>: í‘œì¤€ì •ê·œë¶„í¬ ì¶”ì¶œ í•¨ìˆ˜
  + í‰ê·  0, í‘œì¤€í¸ì°¨ 1ì„ ê°€ì§€ëŠ” í‘œì¤€ì •ê·œë¶„í¬ ë‚´ì—ì„œ ì„ì˜ ì¶”ì¶œí•œë‹¤
  ```python
  np.random.randn() # -0.13049944410660697
  np.random.randn(4) # array([ 0.68319741, -1.37804659,  1.25332479, -1.21334304])
  np.random.randn(4, 1)
  np.random.randn(3, 2)
  '''
  array([[-0.2243001 ],
       [ 0.03057147],
       [-1.26080828],
       [-1.94113049]])
  array([[ 0.39715935,  0.27487829],
       [-0.9506768 , -0.0807032 ],
       [-0.97761767,  0.92817657]])
  '''
  ```
+ <code>randint()</code>: ì •ìˆ˜ ì„ì˜ ì¶”ì¶œ í•¨ìˆ˜
  + ì¸ì: (lowerbound, highbound, size)
  + lowerbound ~ highbound-1 ì‚¬ì´ ë²”ìœ„ì—ì„œ ì •ìˆ˜ë¥¼ ì„ì˜ ì¶”ì¶œí•œë‹¤
  ```python
    np.random.randint(-2, 2, 3) # array([-1, -2, -2])
    np.random.randint(1, 8, 5) # array([3, 5, 4, 5, 3])
  ```


### íŒŒì´ì¬ í–‰ë ¬ ê³±ì—°ì‚° ì •ë¦¬
+ 1ï¸âƒ£ í–‰ë ¬ A, Bê°€ `np.array()` í˜•íƒœì¼ ë•Œ
  + ë³„ì—°ì‚°(`*`): ìŠ¤ì¹¼ë¼ê³±
    + (n,m)*(n,m) = (n,m)
    + ë¸Œë¡œë“œìºìŠ¤íŒ…ì‹œ
      (1,m)*(n,m) = (n,m)
      or (m,1)*(m,n) = (m,n)
      or (m,n)*(m,1) = (m,n)
      or (n,m)*(1,m) = (n,m)
  + ë‚´ì (`dot`): ì„ í˜•ëŒ€ìˆ˜ì˜ ë‚´ì ê³¼ ê°™ë‹¤
    + (n,m).dot((m,k)) = (n,k)
  + í–‰ë ¬ê³±(`@`): í–‰ë ¬ì˜ ê³±ì…ˆ
    + ë‹¨, 2ì°¨ì› ê³µê°„(í–‰ë ¬)ì—ì„œëŠ” ë‚´ì ê³¼ ê°™ì€ ì—­í• ì„ í•œë‹¤
    + 3ì°¨ì›ë¶€í„°ëŠ” tensor ê³±(ì™¸ì )ì„ í•˜ê²Œ ëœë‹¤
    + (n,m)@(m,k) = (n,k)
+ 2ï¸âƒ£ í–‰ë ¬ A, Bê°€ `np.matrix()` í˜•íƒœì¼ ë•Œ



### Broadcasting
+ <code>Broadcasting</code>: an operation of matching the dimensions of differently shaped arrays in order to be able to perform further operations on those arrays
+ ì°¨ì›ì˜ í¬ê¸°ê°€ 1ì´ê±°ë‚˜, ì°¨ì›ì— ëŒ€í•´ ì¶•ì˜ ê¸¸ì´ê°€ ë™ì¼í•  ë•Œ ê°€ëŠ¥í•˜ë‹¤
+ code execution timeì„ ì¤„ì¼ ìˆ˜ ìˆëŠ” ê¸°ë²•ì´ë‹¤
+ dimensionì´ auto expanded ëœë‹¤
<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/887eab24-ed9b-463e-a761-e191c9f7aa9e">

### Broadcasting Examples
+ Calories from Carbs, Proteins, Fats in 100g of different foods<br>
  <img width="427" alt="image" src="https://github.com/sml09181/sml09181.github.io/assets/105408672/452858cf-b559-421b-9adc-3cddbbdaec9e">
  ```python
    A = np.array([[56.0, 0.0, 4.4, 68.0],
                [1.2, 104.0, 52.0, 8.0],
                [1.8, 135.0, 99.0, 0.9]])
    cal = A.sum(axis=0) 
    print(cal) # [ 59.  239.  155.4  76.9]
    print(cal.shape) # (4,)
    percentage = 100*A/cal.reshape(1,4)
    print(percentage)
    '''
    [[94.91525424  0.          2.83140283 88.42652796]
    [ 2.03389831 43.51464435 33.46203346 10.40312094]
    [ 3.05084746 56.48535565 63.70656371  1.17035111]]
    '''
  ```
+ ë§ì…ˆ<br>
  <img width="398" alt="image" src="https://github.com/sml09181/sml09181.github.io/assets/105408672/2338425f-0ccf-4f7b-bbcf-6595d3f49682">
+ General Principle<br>
  <img width="400" alt="image" src="https://github.com/sml09181/sml09181.github.io/assets/105408672/fdf5707f-f5a8-49b9-8489-ddbf149a4835">
+ í€´ì¦ˆ
  + í€´ì¦ˆ1
  ```python
    a = np.random.randn(2, 3) # a.shape = (2, 3)
    b = np.random.randn(2, 1) # b.shape = (2, 1)
    c = a * b
    print(c)
    print(c.shape)
    '''
    [[-0.24956183  0.31327135  1.00597402]
    [ 1.13608675 -1.01161003  0.14922026]]
    (2, 3)
    '''
  ```
  + í€´ì¦ˆ2
  ```python
    a = np.random.randn(4, 3)
    b = np.random.randn(3, 2)
    c = a * b
    '''
    ì‚¬ì´ì¦ˆê°€ ë§¤ì¹˜ë˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì—ëŸ¬ê°€ ë‚œë‹¤.
    ValueError: operands could not be broadcast together with shapes (4,3) (3,2) 
    '''
  ```
  + í€´ì¦ˆ3
  ```python
    a = np.random.randn(3, 3)
    b = np.random.randn(3, 1)
    c = a * b
    print(c)
    print(c.shape)
    '''
    [[ 1.62479277  1.82880562  0.94845195]
    [ 0.01090171 -0.26786701  0.3190328 ]
    [-0.40082135 -0.24469142  0.55287189]]
    (3, 3)
    '''
  ```
  + í€´ì¦ˆ4
  ```python
    a = np.random.randn(2, 3)
    print(a)
    print(a.shape)
    b = np.random.randn(2, 1)
    print(b)
    print(b.shape)
    c = a + b
    print(c)
    print(c.shape)
    '''
    [[-1.40759913  0.70449155 -0.53921712]
    [-0.03529759  0.25307406  2.94068672]]
    (2, 3)
    [[-0.01688486]
    [-0.19458319]]
    (2, 1)
    [[-1.42448398  0.68760669 -0.55610198]
    [-0.22988078  0.05849087  2.74610353]]
    (2, 3)
    '''
  ```

## A Note on Python
+ ê²°ë¡ : <mark style='background-color: #f5f0ff'>ğŸŒŸRankê°€ 1ì¸ í˜•íƒœë¥¼ ì“°ì§€ ë§ìğŸŒŸ</mark>
  + `(n, )` ê°™ì´ Rank 1ì¸ í˜•íƒœì˜ ìë£Œêµ¬ì¡°ëŠ” í–‰ë²¡í„°ë„ ì•„ë‹ˆê³  ì—´ë²¡í„°ë„ ì•„ë‹ˆë¯€ë¡œ, ì§ê´€ì ì´ì§€ ì•Šì€ ê²°ê³¼ë¥¼ ë„ì¶œí•œë‹¤
+ <code>shape</code>: í–‰ë ¬ì˜ í˜•íƒœë¥¼ ë³¼ ìˆ˜ ìˆë‹¤(í–‰, ì—´)
+ <code>assert</code>: í•´ë‹¹ ì½”ë“œ ë¶€ë¶„ì—ì„œ ì–´ë–¤ ì¡°ê±´ì´ ì°¸ì„ì„ í™•ê³ íˆ í•˜ëŠ” ê²ƒ
  + í˜•ì‹: `assert [ì¡°ê±´ì‹], [ë©”ì‹œì§€]`
  + ì¡°ê±´ë¬¸ì´ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ [ë©”ì‹œì§€]ë¥¼ ë‹´ì•„ AssertionError ë°œìƒì‹œí‚¨ë‹¤
  + í–‰ë ¬ê³¼ ë°°ì—´ì˜ ì°¨ì›ì„ í™•ì¸í•  ë•Œ ì‚¬ìš©í•˜ì
  + rank1ë¥¼ ì–»ê²Œ ë˜ë©´ <code>reshape</code>ìœ¼ë¡œ í–‰ë²¡í„° ë˜ëŠ” ì—´ë²¡í„°ë¡œ ë°”ê¿”ì£¼ì
  + Examples
    ```python
      assert image.shape == [3, 224, 224]
      assert image.shape[0] == 3
      assert array1.shape[0] == array2.shape[0]
      assert len(array2.shape) == 3
    ```
    ```python
    # ì¶œì²˜: https://hbase.tistory.com/398
    def test(age):
      assert type(age) is int, 'age ê°’ì€ ì •ìˆ˜ë§Œ ê°€ëŠ¥'
      assert age > 0, 'age ê°’ì€ ì–‘ìˆ˜ë§Œ ê°€ëŠ¥'

    age = 1
    test(age)

    age = -10
    test(age)

    # Traceback (most recent call last):
    #   File "./test.py", line 12, in <module>
    #     test(age)
    #   File "./test.py", line 6, in test
    #     assert age > 0, 'age ê°’ì€ ì–‘ìˆ˜ë§Œ ê°€ëŠ¥'
    # AssertionError: age ê°’ì€ ì–‘ìˆ˜ë§Œ ê°€ëŠ¥
    ```


## Explanation of Logistic Regression Cost Function

### Logistic regression cost function
+ 1ï¸âƒ£ Logistic Regressionì—ì„œ ë°°ìš´ ì†ì‹¤í•¨ìˆ˜
  + $y$ ê°’ì´ 1ì´ ë  í™•ë¥ : $P(y=1 \mid x) = \hat y$
  + $y$ ê°’ì´ 0ì´ ë  í™•ë¥ : $P(y=0 \mid x) = 1- \hat y$
+ 2ï¸âƒ£ ìœ„ ë‘ ë“±ì‹ì„ í•˜ë‚˜ì˜ ìˆ˜ì‹ìœ¼ë¡œ í•©ì³ë³´ì
  + $P(y \mid x) = \hat y^y(1-\hat y)^{(1-y)}$
  + if $y=1$: $P(y \mid x) = \hat y^1(1-\hat y)^0$
  + if $y=0$: $P(y \mid x) = \hat y^0(1-\hat y)^1$
+ 3ï¸âƒ£ ë¡œê·¸í•¨ìˆ˜ëŠ” ë‹¨ì¡° ì¦ê°€ í•¨ìˆ˜ì´ë¯€ë¡œ ìœ„ ì‹ì€ ì•„ë˜ì™€ ë™ì¼í•˜ë‹¤<br>
  $\log P(y \mid x) = \log(\hat y^y(1-\hat y)^{(1-y)})
  \\ = y\log \hat y + (1-y)\log(1-\hat y) $
+ 4ï¸âƒ£ Our Goal: í™•ë¥  $\log P(y \mid x)$ ì„ ìµœëŒ€í™”ì‹œí‚¤ëŠ” ê²ƒ â–¶ï¸ ì†ì‹¤í•¨ìˆ˜ $-\log P(y \mid x)$ ì„ ìµœì†Œí™”ì‹œí‚¤ì
+ 5ï¸âƒ£ training sample í•˜ë‚˜ì˜ ì†ì‹¤í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤
  $ L(\hat y, y) = -\log P(y \mid x) = y\log \hat y + (1-y)\log(1-\hat y) $


### Cost on m examples
+ training samplesì´ `IID`(independently and identically distributed)ë¼ ê°€ì •í•œë‹¤
  + `IID`: ì„œë¡œ ë…ë¦½ì´ê³  ê°ê° ë™ì¼í•œ í™•ë¥ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” ë‹¤ì°¨ì› í™•ë¥ ë³€ìˆ˜
  + ë™ì¼í•œ ì¡°ê±´ ì•„ë˜ì—ì„œ ìˆ˜í–‰ë˜ëŠ” ì‹¤í—˜ì´ë‚˜ ê´€ì¸¡ì„ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•˜ì—¬ ë°ì´í„°ë¥¼ ì–»ëŠ” ê²ƒ
+ scaleì„ ë§ì¶”ê¸° ìœ„í•´ `m`ìœ¼ë¡œ ë‚˜ëˆ ì¤€ë‹¤
+ Cost: $J(w, b)= \frac{1}{m} \sum_{i=1}^{m} L(\hat y^{(i)}, y^{(i)})$
<img width="629" alt="image" src="https://github.com/sml09181/sml09181.github.io/assets/105408672/386c5802-6084-4f31-806c-9fb6f093c75a">

<br><br>

Latexë¡œ ì—´ë²¡í„° ì…ë ¥í•˜ëŠ” ê±° ë„ˆë¬´ ì–´ë µë‹¤..

<br><br>

Source:
+ [ğŸŒŸ ë„˜íŒŒì´ ëœë¤ ì¶”ì¶œ í•¨ìˆ˜ ì •ë¦¬ : rand, random, randn, randint, choice, seed](https://jimmy-ai.tistory.com/60)
+ [ğŸŒŸ Python assert ì‚¬ìš©ë²• ë° ì˜ˆì œ](https://hbase.tistory.com/398)
+ [Assertion of arbitrary array shapes in Python](https://medium.com/@nearlydaniel/assertion-of-arbitrary-array-shapes-in-python-3c96f6b7ccb4)
+ [9-1. ë…ë¦½ë™ì¼ë¶„í¬(ë…ë¦½ì„±, í•©ì˜ ë¶„í¬)](https://data-science-note.tistory.com/entry/9-1-%EB%8F%85%EB%A6%BD%EB%8F%99%EC%9D%BC%EB%B6%84%ED%8F%AC%EB%8F%85%EB%A6%BD%EC%84%B1-%ED%95%A9%EC%9D%98-%EB%B6%84%ED%8F%AC#:~:text=%EB%8F%85%EB%A6%BD%EB%8F%99%EC%9D%BC%EB%B6%84%ED%8F%AC(i.i.d.%3B%20independently,%ED%95%98%EC%97%AC%20%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A5%BC%20%EC%96%BB%EB%8A%94%20%EA%B2%83.))
+ [[Numpy] Broadcasting(ë¸Œë¡œë“œìºìŠ¤íŒ…)](https://seong6496.tistory.com/54)
+ [[Numpy]í–‰ë ¬ê³±(@)ê³¼ ë‚´ì (dot) ê·¸ë¦¬ê³  ë³„ì—°ì‚°(*)](https://seong6496.tistory.com/110)
+ [ğŸŒŸ [Numpy] íŒŒì´ì¬ ë‚´ì , í–‰ë ¬ê³± í•¨ìˆ˜ np.dot() ì‚¬ìš©ë²• ì´ì •ë¦¬](https://jimmy-ai.tistory.com/75) -> ë‚˜ì¤‘ì— ë‹¤ì‹œ ê³µë¶€