---
title: Deep Learning Specialization 1-2 | Basics of Neural Network Programming
author: Su
date: 2023-09-11 05:55:00 +0800
categories: [DL]
tags: [EURON]
pin: false
use_math: true

---

Learning Source
+ [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning?utm_source=deeplearningai&utm_medium=institutions&utm_campaign=SocialYoutubeDLSC1W1L1#courses)
+ [부스트코스 딥러닝 1단계: 신경망과 딥러닝](https://m.boostcourse.org/ai215/lectures/86249)

# **Basics of Neural Network Programming**

## Binary Classification
<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/430837bc-f422-4f38-a09d-cc6d130b585f" width="600">

+ 신경망이 학습하는 방법
  + Feedfoward(순전파)
  + BackPropagation(역전파)
+ Notation
  + 하나의 training sample $(x, y)$: $x \in \mathbb{R}^{n_x}, y \in {0, 1}$
  + m개의 training samples: ${(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ... , (x^{(m)}, y^{(m)})}$<br>
  $ X = \begin{bmatrix} \mid & \mid & 0 & \mid \\ X^{(1)} & X^{(2)} & ... & X^{(m)} \\ \mid & \mid & 0 & \mid \\  \end{bmatrix} $
    + 이때 NN에서는 stack $X$ by column이 보편적이다.
    + $X \in \mathbb{R}^{n_x \times m}$
    + $X.shape = (n_x, m)$
  + $Y=[y^{(1)}, y^{(2)}, ..., y^{(m)}]$
    + 이때 NN에서는 stack $y$ by column이 보편적이다.
    + $y \in \mathbb{R}^{1 \times m}$
    + $y.\operatorname{shape} = (1, m)$

+ <code>Binary Classification</code>(이진 분류): 그렇다 / 아니다 2개로 분류하는 것
  + 그렇다 ➡️ 1 / 아니다 ➡️ 0
    + ex) 고양이다 / 고양이가 아니다.
  + 고양이 사진
    + feature vector: 모든 pixel 값을 feature vector $X$의 한 열로 나열하여 만든다.

    + $n_x$: dimension of input feature vector


## Logistic Regression

<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/b05ad3bd-8cd5-43b4-81d3-84e93c2b8944" width="600"/>

+ <code>Logistic Regression</code>(로지스틱 회귀): 답이 0 또는 1로 정해져있는 이진 분류 문제에 사용되는 알고리즘
+ Given feature vector $x(x \in \mathbb{R}^{n_x}) $, want $\hat y = P(y=1 \mid x)$ where $0 \leq \hat y \leq 1$
+ $X$:입력 특성, $y$:주어진 입력특성 $X$ 에 해당하는 실제 값, $\hat y$: $y$의 예측값
+ 1️⃣ 선형 회귀 $ \hat y = W^TX + b $ 로 계산
  + $\hat y $: $y$가 1일 확률
  + binary classfication을 위해서는 $\hat y $ 는 0과 1 사이의 값을 가져야 하지만, 해당 값은 해당 범위를 벗어날 수 있다. ➡️ sigmoid 함수 이용
+ 2️⃣ Logistic Regression를 위해 $ \hat y = \sigma(W^TX+b) $ 로 0과 1 사이로 범위를 제한한다.
  + sigmoid 함수: $ \sigma(z) = \frac{1}{1+e^{-z}} $
    + if $z$ large **positive** number: $ \sigma (z) \approx \frac {1}{1+0} \approx 1 $
    + if $z$ large **negative** number: $ \sigma (z) \approx \frac {1}{1+ \operatorname{BigNumber}} \approx 0 $
  <img width="400" src="https://github.com/sml09181/sml09181.github.io/assets/105408672/31b54603-c7ae-4200-b3e0-ec219bff8b3e" />

## Logistic Regression Cost Function
+ Our Goal: prediction($\hat y$)을 ground truth($y$)에 최대한 가깝게 구하는 것
+ <code>Loss function</code>(Error Function, 손실함수)
  + for **a** train sample
  + 하나의 입력 특성($x$)에 대한 실제값($y$)과 예측값($ \hat y$)을 계산하는 함수
  + 일반적인 loss function: $L(\hat y, y) = \frac{1}{2}(\hat y - y)^2$ 
    + 🌟 그러나 Logistic Regression에서는 위 loss function을 사용하면 local optimum problem에 빠질 수 있기 때문에 별도의 loss function을 사용한다.
  
### Lost Function in Logistic Regression:
$$ L(\hat y, y) = -(y\log \hat y + (1-y)\log (1-\hat y)) $$

+ 1️⃣ $y=0인 경우$: $L(\hat y, y) = -(y\log \hat y + (1-y)\log (1-\hat y))$
+ 2️⃣ $y=1인 경우$: $L(\hat y, y) = -(y\log \hat y + (1-y)\log (1-\hat y))$
+ <code>Cost function</code>(비용 함수)
  + for **all** samples of train set
  + 모든 입력에 대한 오차를 계산한다.
  + 모든 입력에 대해 계산한 loss function의 평균이다.
  + $J(w, b) = - \frac{1}{m} \sum_{i=1}^{i=m} (y^(i)\log \hat y^{(i)} + (1-y^{(i)} \log(1-\hat y^{(i)}))) $


+ So, Our Goal: cost function을 최소화하는 $ w$ 와 $ b $ 찾기

## Gradient Descent
<img width="400" src="https://github.com/sml09181/sml09181.github.io/assets/105408672/8bbbb686-414a-4c96-9b37-314780df8670">

+ <code>Cost Function</code>: 전체 데이터셋의 예측이 얼마나 잘 평가되었는지 볼 수 있다.
  + Recap
    + $ \hat y = \sigma(W^TX+b) $, $ \sigma(z) = \frac{1}{1+e^{-z}} $
    + $J(w, b) = - \frac{1}{m} \sum_{i=1}^{i=m} (y^(i)\log \hat y^{(i)} + (1-y^{(i)} \log(1-\hat y^{(i)}))) $
  + 경사하강법으로 최적의 파라미터를 찾기 위해서는 **convex function**(볼록 함수)여야 한다.
+ <code>Gradient Descent</code>(경사하강법): Cost Function $ J(w, b) $을 최소화시키는 parameter $w$와 $b$를 찾아낸다.
  + 임의로 initialize(보통은 0 사용)
  + convex하기 때문에 어느 지점에서 시작해도 global optinum에 도달하게 된다.
  + 경사하강법은 가장 가파른(steepest) 방향, 즉 함수의 기울기를 따라서 최적의 값으로 한 스텝씩 업데이트하게 된다.
  + $ \alpha $(learning rate): 학습률이라고 하며, 얼만큼의 스텝으로 나아갈 것인지 정한다.
  + $\frac{dJ(w)}{dw}$: 도함수(함수의 기울기)라고 하며, 미분을 통해 구한 값이다.
    + in code: `dw`
    + if `dw >0`: parameter $w$ 는 기존의 $w$ 값 보다 작은 방향으로 업데이트
    + if `dw <0`: parameter $w$ 는 기본의 $w$ 값 보다 큰 방향으로 업데이트 
    + $dw = \frac{\partial J(w, b)}{\partial w}$: 함수의 기울기가 $w$ 방향으로 얼만큼 변했는지를 나타낸다.
    + $db = \frac{\partial J(w, b)}{\partial b}$: 함수의 기울기가 $b$ 방향으로 얼만큼 변했는지를 나타낸다.


## Derivatives
+ <code>Derivatives</code>(=도함수, 어떤 함수의 기울기): 변수 $a$를 조금만 변화시켰을 때, 함수 $f(a)$가 얼만큼 변하는지를 측정하는 것
+ 표기법: $\frac{d}{da}f(a) = \frac{d(fa)}{da}$

<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/497654b6-4997-4a6e-ad11-8e3ea0ae3b90" >

## Computation Graph
+ $J(a,b,c)=3(a+bc)$ 의 Computation Graph 만들기
  + $u = bc$
  + $v = a+u$
  + $J = 3v$
<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/fc29b1de-2078-4d61-a5fd-743fceb07c03">

## Derivatives with a Computation Graph
+ <code>Chain Rule</code>(미분의 연쇄법칙)
  + 합성함수의 도함수에 대한 공식
  + 합성함수를 구성하는 함수의 미분을 곱하여 구한다.
  + 입력변수 a 를 통해서 출력변수 J 까지 도달하기 위해서  a→v→J  의 프로세스로 진행된다. 즉, 변수 a 만 보게 된다면, J = J(u(a)) 라는 합성함수가 될 것이다.
  + $ \frac{dJ}{da} = \frac{dJ}{dv} \times \frac{dv}{da}$
+ code 작성시 편의를 위해서 표기법을 아래와 같이 정의
  + 최종변수: `FinalOutputVar`
  + 미분하려고 하는 변수: `Var`
  + $ \frac{d FinalOutputVar}{d Var} = d Var$

## Logistic Regression Gradient descent
+ 그래프
  <img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/652a48fe-e90b-480a-a509-8009b633c885">
+ 수식
  + $da = - \frac{y}{a} + \frac{1-y}{1-a}$
  + $dz = a - y$
  + $dw_1 = \frac{dL}{dw_1} = x_1dz$
  + $db = \frac{dL}{db} = dz$

## Gradient descent on *m* examples
+ Cost Function in Logistic Regression
  + $ J(w, b) = \frac{1}{m} \sum_{i=1}^{i=m}(L(a^{(i)}, y^{(i)})) $

+ 현재 코드에서는 feature의 개수를 2개로 가정하였지만, 만약 특성의 개수가 많아진다면 이 또한 for문을 이용해 처리해야 한다. 
+ 즉, 이중 for문을 사용하게 되며 이로 인해 계산속도가 느려진다.

<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/e4ba8ad3-f6b9-484f-802f-b273f6a19c08" width="500">


Source
+ https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21
+ https://www.quora.com/Can-we-use-logistic-regression-for-continuous-variables
+ https://datahacker.rs/gradient-descent-neural-networks/