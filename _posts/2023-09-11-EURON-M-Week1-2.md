---
title: Deep Learning Specialization 1-2 | Basics of Neural Network Programming
author: Su
date: 2023-09-11 05:55:00 +0800
categories: [DL]
tags: [EURON]
pin: false
use_math: true

---

Learning Source
+ [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning?utm_source=deeplearningai&utm_medium=institutions&utm_campaign=SocialYoutubeDLSC1W1L1#courses)
+ [ë¶€ìŠ¤íŠ¸ì½”ìŠ¤ ë”¥ëŸ¬ë‹ 1ë‹¨ê³„: ì‹ ê²½ë§ê³¼ ë”¥ëŸ¬ë‹](https://m.boostcourse.org/ai215/lectures/86249)

# **Basics of Neural Network Programming**

## Binary Classification
<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/430837bc-f422-4f38-a09d-cc6d130b585f" width="600">

+ ì‹ ê²½ë§ì´ í•™ìŠµí•˜ëŠ” ë°©ë²•
  + Feedfoward(ìˆœì „íŒŒ)
  + BackPropagation(ì—­ì „íŒŒ)
+ Notation
  + í•˜ë‚˜ì˜ training sample $(x, y)$: $x \in \mathbb{R}^{n_x}, y \in {0, 1}$
  + mê°œì˜ training samples: ${(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ... , (x^{(m)}, y^{(m)})}$<br>
  $ X = \begin{bmatrix} \mid & \mid & 0 & \mid \\ X^{(1)} & X^{(2)} & ... & X^{(m)} \\ \mid & \mid & 0 & \mid \\  \end{bmatrix} $
    + ì´ë•Œ NNì—ì„œëŠ” stack $X$ by columnì´ ë³´í¸ì ì´ë‹¤.
    + $X \in \mathbb{R}^{n_x \times m}$
    + $X.shape = (n_x, m)$
  + $Y=[y^{(1)}, y^{(2)}, ..., y^{(m)}]$
    + ì´ë•Œ NNì—ì„œëŠ” stack $y$ by columnì´ ë³´í¸ì ì´ë‹¤.
    + $y \in \mathbb{R}^{1 \times m}$
    + $y.\operatorname{shape} = (1, m)$

+ <code>Binary Classification</code>(ì´ì§„ ë¶„ë¥˜): ê·¸ë ‡ë‹¤ / ì•„ë‹ˆë‹¤ 2ê°œë¡œ ë¶„ë¥˜í•˜ëŠ” ê²ƒ
  + ê·¸ë ‡ë‹¤ â¡ï¸ 1 / ì•„ë‹ˆë‹¤ â¡ï¸ 0
    + ex) ê³ ì–‘ì´ë‹¤ / ê³ ì–‘ì´ê°€ ì•„ë‹ˆë‹¤.
  + ê³ ì–‘ì´ ì‚¬ì§„
    + feature vector: ëª¨ë“  pixel ê°’ì„ feature vector $X$ì˜ í•œ ì—´ë¡œ ë‚˜ì—´í•˜ì—¬ ë§Œë“ ë‹¤.

    + $n_x$: dimension of input feature vector


## Logistic Regression

<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/b05ad3bd-8cd5-43b4-81d3-84e93c2b8944" width="600"/>

+ <code>Logistic Regression</code>(ë¡œì§€ìŠ¤í‹± íšŒê·€): ë‹µì´ 0 ë˜ëŠ” 1ë¡œ ì •í•´ì ¸ìˆëŠ” ì´ì§„ ë¶„ë¥˜ ë¬¸ì œì— ì‚¬ìš©ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜
+ Given feature vector $x(x \in \mathbb{R}^{n_x}) $, want $\hat y = P(y=1 \mid x)$ where $0 \leq \hat y \leq 1$
+ $X$:ì…ë ¥ íŠ¹ì„±, $y$:ì£¼ì–´ì§„ ì…ë ¥íŠ¹ì„± $X$ ì— í•´ë‹¹í•˜ëŠ” ì‹¤ì œ ê°’, $\hat y$: $y$ì˜ ì˜ˆì¸¡ê°’
+ 1ï¸âƒ£ ì„ í˜• íšŒê·€ $ \hat y = W^TX + b $ ë¡œ ê³„ì‚°
  + $\hat y $: $y$ê°€ 1ì¼ í™•ë¥ 
  + binary classficationì„ ìœ„í•´ì„œëŠ” $\hat y $ ëŠ” 0ê³¼ 1 ì‚¬ì´ì˜ ê°’ì„ ê°€ì ¸ì•¼ í•˜ì§€ë§Œ, í•´ë‹¹ ê°’ì€ í•´ë‹¹ ë²”ìœ„ë¥¼ ë²—ì–´ë‚  ìˆ˜ ìˆë‹¤. â¡ï¸ sigmoid í•¨ìˆ˜ ì´ìš©
+ 2ï¸âƒ£ Logistic Regressionë¥¼ ìœ„í•´ $ \hat y = \sigma(W^TX+b) $ ë¡œ 0ê³¼ 1 ì‚¬ì´ë¡œ ë²”ìœ„ë¥¼ ì œí•œí•œë‹¤.
  + sigmoid í•¨ìˆ˜: $ \sigma(z) = \frac{1}{1+e^{-z}} $
    + if $z$ large **positive** number: $ \sigma (z) \approx \frac {1}{1+0} \approx 1 $
    + if $z$ large **negative** number: $ \sigma (z) \approx \frac {1}{1+ \operatorname{BigNumber}} \approx 0 $
  <img width="400" src="https://github.com/sml09181/sml09181.github.io/assets/105408672/31b54603-c7ae-4200-b3e0-ec219bff8b3e" />

## Logistic Regression Cost Function
+ Our Goal: prediction($\hat y$)ì„ ground truth($y$)ì— ìµœëŒ€í•œ ê°€ê¹ê²Œ êµ¬í•˜ëŠ” ê²ƒ
+ <code>Loss function</code>(Error Function, ì†ì‹¤í•¨ìˆ˜)
  + for **a** train sample
  + í•˜ë‚˜ì˜ ì…ë ¥ íŠ¹ì„±($x$)ì— ëŒ€í•œ ì‹¤ì œê°’($y$)ê³¼ ì˜ˆì¸¡ê°’($ \hat y$)ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
  + ì¼ë°˜ì ì¸ loss function: $L(\hat y, y) = \frac{1}{2}(\hat y - y)^2$ 
    + ğŸŒŸ ê·¸ëŸ¬ë‚˜ Logistic Regressionì—ì„œëŠ” ìœ„ loss functionì„ ì‚¬ìš©í•˜ë©´ local optimum problemì— ë¹ ì§ˆ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë³„ë„ì˜ loss functionì„ ì‚¬ìš©í•œë‹¤.
  
### Lost Function in Logistic Regression:
$$ L(\hat y, y) = -(y\log \hat y + (1-y)\log (1-\hat y)) $$

+ 1ï¸âƒ£ $y=0ì¸ ê²½ìš°$: $L(\hat y, y) = -(y\log \hat y + (1-y)\log (1-\hat y))$
+ 2ï¸âƒ£ $y=1ì¸ ê²½ìš°$: $L(\hat y, y) = -(y\log \hat y + (1-y)\log (1-\hat y))$
+ <code>Cost function</code>(ë¹„ìš© í•¨ìˆ˜)
  + for **all** samples of train set
  + ëª¨ë“  ì…ë ¥ì— ëŒ€í•œ ì˜¤ì°¨ë¥¼ ê³„ì‚°í•œë‹¤.
  + ëª¨ë“  ì…ë ¥ì— ëŒ€í•´ ê³„ì‚°í•œ loss functionì˜ í‰ê· ì´ë‹¤.
  + $J(w, b) = - \frac{1}{m} \sum_{i=1}^{i=m} (y^(i)\log \hat y^{(i)} + (1-y^{(i)} \log(1-\hat y^{(i)}))) $


+ So, Our Goal: cost functionì„ ìµœì†Œí™”í•˜ëŠ” $ w$ ì™€ $ b $ ì°¾ê¸°

## Gradient Descent
<img width="400" src="https://github.com/sml09181/sml09181.github.io/assets/105408672/8bbbb686-414a-4c96-9b37-314780df8670">

+ <code>Cost Function</code>: ì „ì²´ ë°ì´í„°ì…‹ì˜ ì˜ˆì¸¡ì´ ì–¼ë§ˆë‚˜ ì˜ í‰ê°€ë˜ì—ˆëŠ”ì§€ ë³¼ ìˆ˜ ìˆë‹¤.
  + Recap
    + $ \hat y = \sigma(W^TX+b) $, $ \sigma(z) = \frac{1}{1+e^{-z}} $
    + $J(w, b) = - \frac{1}{m} \sum_{i=1}^{i=m} (y^(i)\log \hat y^{(i)} + (1-y^{(i)} \log(1-\hat y^{(i)}))) $
  + ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ê¸° ìœ„í•´ì„œëŠ” **convex function**(ë³¼ë¡ í•¨ìˆ˜)ì—¬ì•¼ í•œë‹¤.
+ <code>Gradient Descent</code>(ê²½ì‚¬í•˜ê°•ë²•): Cost Function $ J(w, b) $ì„ ìµœì†Œí™”ì‹œí‚¤ëŠ” parameter $w$ì™€ $b$ë¥¼ ì°¾ì•„ë‚¸ë‹¤.
  + ì„ì˜ë¡œ initialize(ë³´í†µì€ 0 ì‚¬ìš©)
  + convexí•˜ê¸° ë•Œë¬¸ì— ì–´ëŠ ì§€ì ì—ì„œ ì‹œì‘í•´ë„ global optinumì— ë„ë‹¬í•˜ê²Œ ëœë‹¤.
  + ê²½ì‚¬í•˜ê°•ë²•ì€ ê°€ì¥ ê°€íŒŒë¥¸(steepest) ë°©í–¥, ì¦‰ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ë”°ë¼ì„œ ìµœì ì˜ ê°’ìœ¼ë¡œ í•œ ìŠ¤í…ì”© ì—…ë°ì´íŠ¸í•˜ê²Œ ëœë‹¤.
  + $ \alpha $(learning rate): í•™ìŠµë¥ ì´ë¼ê³  í•˜ë©°, ì–¼ë§Œí¼ì˜ ìŠ¤í…ìœ¼ë¡œ ë‚˜ì•„ê°ˆ ê²ƒì¸ì§€ ì •í•œë‹¤.
  + $\frac{dJ(w)}{dw}$: ë„í•¨ìˆ˜(í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°)ë¼ê³  í•˜ë©°, ë¯¸ë¶„ì„ í†µí•´ êµ¬í•œ ê°’ì´ë‹¤.
    + in code: `dw`
    + if `dw >0`: parameter $w$ ëŠ” ê¸°ì¡´ì˜ $w$ ê°’ ë³´ë‹¤ ì‘ì€ ë°©í–¥ìœ¼ë¡œ ì—…ë°ì´íŠ¸
    + if `dw <0`: parameter $w$ ëŠ” ê¸°ë³¸ì˜ $w$ ê°’ ë³´ë‹¤ í° ë°©í–¥ìœ¼ë¡œ ì—…ë°ì´íŠ¸ 
    + $dw = \frac{\partial J(w, b)}{\partial w}$: í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ê°€ $w$ ë°©í–¥ìœ¼ë¡œ ì–¼ë§Œí¼ ë³€í–ˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.
    + $db = \frac{\partial J(w, b)}{\partial b}$: í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ê°€ $b$ ë°©í–¥ìœ¼ë¡œ ì–¼ë§Œí¼ ë³€í–ˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.


## Derivatives
+ <code>Derivatives</code>(=ë„í•¨ìˆ˜, ì–´ë–¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°): ë³€ìˆ˜ $a$ë¥¼ ì¡°ê¸ˆë§Œ ë³€í™”ì‹œì¼°ì„ ë•Œ, í•¨ìˆ˜ $f(a)$ê°€ ì–¼ë§Œí¼ ë³€í•˜ëŠ”ì§€ë¥¼ ì¸¡ì •í•˜ëŠ” ê²ƒ
+ í‘œê¸°ë²•: $\frac{d}{da}f(a) = \frac{d(fa)}{da}$

<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/497654b6-4997-4a6e-ad11-8e3ea0ae3b90" >

## Computation Graph
+ $J(a,b,c)=3(a+bc)$ ì˜ Computation Graph ë§Œë“¤ê¸°
  + $u = bc$
  + $v = a+u$
  + $J = 3v$
<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/fc29b1de-2078-4d61-a5fd-743fceb07c03">

## Derivatives with a Computation Graph
+ <code>Chain Rule</code>(ë¯¸ë¶„ì˜ ì—°ì‡„ë²•ì¹™)
  + í•©ì„±í•¨ìˆ˜ì˜ ë„í•¨ìˆ˜ì— ëŒ€í•œ ê³µì‹
  + í•©ì„±í•¨ìˆ˜ë¥¼ êµ¬ì„±í•˜ëŠ” í•¨ìˆ˜ì˜ ë¯¸ë¶„ì„ ê³±í•˜ì—¬ êµ¬í•œë‹¤.
  + ì…ë ¥ë³€ìˆ˜ a ë¥¼ í†µí•´ì„œ ì¶œë ¥ë³€ìˆ˜ J ê¹Œì§€ ë„ë‹¬í•˜ê¸° ìœ„í•´ì„œ  aâ†’vâ†’J  ì˜ í”„ë¡œì„¸ìŠ¤ë¡œ ì§„í–‰ëœë‹¤. ì¦‰, ë³€ìˆ˜ a ë§Œ ë³´ê²Œ ëœë‹¤ë©´, J = J(u(a)) ë¼ëŠ” í•©ì„±í•¨ìˆ˜ê°€ ë  ê²ƒì´ë‹¤.
  + $ \frac{dJ}{da} = \frac{dJ}{dv} \times \frac{dv}{da}$
+ code ì‘ì„±ì‹œ í¸ì˜ë¥¼ ìœ„í•´ì„œ í‘œê¸°ë²•ì„ ì•„ë˜ì™€ ê°™ì´ ì •ì˜
  + ìµœì¢…ë³€ìˆ˜: `FinalOutputVar`
  + ë¯¸ë¶„í•˜ë ¤ê³  í•˜ëŠ” ë³€ìˆ˜: `Var`
  + $ \frac{d FinalOutputVar}{d Var} = d Var$

## Logistic Regression Gradient descent
+ ê·¸ë˜í”„
  <img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/652a48fe-e90b-480a-a509-8009b633c885">
+ ìˆ˜ì‹
  + $da = - \frac{y}{a} + \frac{1-y}{1-a}$
  + $dz = a - y$
  + $dw_1 = \frac{dL}{dw_1} = x_1dz$
  + $db = \frac{dL}{db} = dz$

## Gradient descent on *m* examples
+ Cost Function in Logistic Regression
  + $ J(w, b) = \frac{1}{m} \sum_{i=1}^{i=m}(L(a^{(i)}, y^{(i)})) $

+ í˜„ì¬ ì½”ë“œì—ì„œëŠ” featureì˜ ê°œìˆ˜ë¥¼ 2ê°œë¡œ ê°€ì •í•˜ì˜€ì§€ë§Œ, ë§Œì•½ íŠ¹ì„±ì˜ ê°œìˆ˜ê°€ ë§ì•„ì§„ë‹¤ë©´ ì´ ë˜í•œ forë¬¸ì„ ì´ìš©í•´ ì²˜ë¦¬í•´ì•¼ í•œë‹¤. 
+ ì¦‰, ì´ì¤‘ forë¬¸ì„ ì‚¬ìš©í•˜ê²Œ ë˜ë©° ì´ë¡œ ì¸í•´ ê³„ì‚°ì†ë„ê°€ ëŠë ¤ì§„ë‹¤.

<img src="https://github.com/sml09181/sml09181.github.io/assets/105408672/e4ba8ad3-f6b9-484f-802f-b273f6a19c08" width="500">


Source
+ https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21
+ https://www.quora.com/Can-we-use-logistic-regression-for-continuous-variables
+ https://datahacker.rs/gradient-descent-neural-networks/