---
title: Ear-VoM_Integrate System to Combat DeepVoice-based Voice Phishing Crimes
author: Su
date: 2024-12-07 11:00:00 +0800
categories: [Personal Project]
tags: [Audio]
pin: false
use_math: true
---


[Github](https://github.com/sml09181/Ear-VoM)

2024년 2학기 18기 이화여자대학교 미래혁신센터의 [도전학기제](https://future.ewha.ac.kr/future/operating-program/introduce02.do) 프로그램에 선정되어 *딥보이스 보이스피싱 범죄 근절을 위한 애플리케이션 탑재용 인공지능 모델 개발*이라는 주제로 개인 프로젝트를 진행하였다.

프로젝트의 주요 성취 실적은 다음과 같다.
- 1️⃣ **First Korean SV2TTS Implementation**
- 2️⃣ **Proposal & Implementation of Deepvoice Detection w/ Federated Learning**
	- Using Deep-Fingerprinting(DF) Model
	- Convert TensorFlow Code to PyTorch (DF)
- 3️⃣ **Proposal of Ear-VoM system (research paper submission)**

이 글에서는 3️⃣ Ear-VoM system에 대해 소개하고, 뒤이은 2개의 글에서 1️⃣, 2️⃣에 대해 서술할 예정이다. 1️⃣, 2️⃣는 3️⃣의 구성 요소로 들어갈 수 있다. 

# 3️⃣ Ear-VoM Overview

Ear-VoM은 딥보이스 보이스피싱 근절을 위해 고안된 예방, 대처·신고, 수사의 세 가지 단계로 구성된 통합 시스템이다. 기존 연구들과 달리 감정을 고려하여 더욱 범죄 상황에 특화된 판별 모델 데이터셋 구축 방법을 포함한다. 

<img width="800" alt="image" src="https://github.com/user-attachments/assets/244ff351-5f9d-4dcb-8307-d37311c26a5c">

각 단계를 간략하게 소개하면 다음과 같다.

- 1️⃣ **예방**
	- TTS로 생성된 가족 및 지인의 딥보이스를 직접 들어본다.
	- 이를 통해 딥보이스의 정교성을 인식한다.
- 2️⃣ **대처·신고**
	- 연합학습으로 꾸준히 업데이트되는 딥보이스 판별 모델
	- 애플리케이션을 통한 국민-경찰과의 긴밀한 소통
- 3️⃣ **수사**
	- 케이-봄(K-VoM): 기존 수사용 한국형 보이스피싱 음성분석 모델
	- MDVF(악의적 딥보이스 핑거프린팅): given 딥보이스의 출처 구분
- ➕ 미리-봄(Miri-VoM) : 자동 딥보이스 보이스피싱 시나리오를 구축하고  오디오 적대적 공격을 활용한 대응책

케이-봄 모델이 수사 도구로써 국민을 지킨다면 이어-봄 시스템은 국민의 동행자로서 인공지능·경찰과 함께 범죄시도 전·중·후 시점에서 국민을 지키도록 고안되었다. ‘이어-봄’의‘이어(Ear)'는 딥보이스 의심 전화를 판별해 낼 수 있는 국민의 ’귀‘라 는 뜻이다.

# 1️⃣Korean SV2TTS Implementation

TTS 논문 중 가장 흥미로었던 InstructTTS를 직접 구현하고 싶었으나, 이를 구현하려면 Expressive TTS보다는 좀 더 basic한 TTS로 먼저 공부하는 게 맞다고 생각했다. 따라서 [SV2TTS](https://arxiv.org/pdf/1806.04558)를 선택하였다. 

*24.12.06 현재 Korean SV2TTS로 검색한 결과 완성된 깃허브 repository는 없다. *

![image](https://github.com/user-attachments/assets/d7ca3709-34fb-428a-b7b6-f570dfc99ec8)



## STEP1: Speech Encoder

### Dataset

- Dataset: [AIHub 화자 인식용 음성 데이터](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=data&dataSetSn=537)
- Preprocessing
	- total 98GB
	- 전체 데이터셋 중 `호출어` 타입은 제외하였다. 
	- sample의 개수가 8개 미만인 speaker는 제외하였다. 

```
Arguments:
    datasets_root:   /GENERATION
    out_dir:         /GENERATION/SV2TTS/encoder
    datasets:        ['enc003']
    skip_existing:   False

Preprocessing enc003
enc003_spkr: Before filtering, there are 2607 speakers.
enc003_spkr: Filter out speakers under 8 samples.
enc003_spkr: Preprocessing data for 2535 speakers.
Done preprocessing enc003_spkr.
```


### Training
[CorentinJ/Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning) 에서의 English weight으로 initialize하였다. 한국어와 언어적인 특성은 다르더라도 성별이나 pitch 등 여러 다양한 요소는 공통된다고 판단했기 때문이다. 
```
Arguments:
    run_id:               encoder
    clean_data_root:      /GENERATION/SV2TTS/encoder
    models_dir:           /GENERATION/result/saved_models/su
    vis_every:            1
    umap_every:           10
    save_every:           10
    backup_every:         500
    force_restart:        False
    visdom_server_port:   8097
    no_visdom:            False

Found existing model "encoder", loading it and resuming training.
Updating the visualizations every 1 steps.
Setting up a new session...
```

### Result

#### Loss & EER

<img width="627" alt="image" src="https://github.com/user-attachments/assets/ab8fba82-b5a9-4ce6-a0c8-9b00d92b5300">

<img width="626" alt="image" src="https://github.com/user-attachments/assets/4409a8bb-7c49-4e2b-a24d-c19bee45b526">


#### Umap
랜덤으로 선택된 9명의 화자의 embedding(d-vector)를 UMAP을 이용하여 차원축소한 결과를 시각화한 것이다. 
아래 그래프는 Encoder 학습 초기의 모습이다. 즉, English weight에서 크게 바뀌지 않은 모습이다. 서로 다른 화자의 임베딩이 뚜렷하게 구분되지 않는 것을 알 수 있다. 크게 두 가지 cluster로 나뉘는데 성별이 그 기준일 거라고 예상한다. 

![image](https://github.com/user-attachments/assets/263d1480-cccb-42cf-9780-36c66665fc1c)

다음 그래프는 Synthesizer에 넣어주기로 선택된 1.565M step에서의 UMAP 결과이다. 같은 화자의 sample은 가까이, 서로 다른 화자의 sample끼리는 멀리 떨어져 있는 것을 확인할 수 있다. 즉, Encoder가 성공적으로 학습되었음을 알 수 있다. 

![image](https://github.com/user-attachments/assets/25c5dba1-98ac-45ef-8ac3-acb2320044a4)

### Visdom

[Visdom](https://ai.meta.com/tools/visdom/)은 rich, live visualization을 위한 tool이다. VS Code SSH remote 연결 상태에서 이 tool을 처음 사용해보아서 처음에 많이 헤맸다. 

에러 해결에 가장 많이 도움이 되었던 블로그는 [여기](https://evjoo.tistory.com/32)이다. 

![image](https://github.com/user-attachments/assets/832bbdeb-9e3f-4a97-bece-f1dd0c69a27a)

학습을 약 5일 동안 진행하였을 때 visdom 창의 모습은 위 사진과 같다. 

## STEP2: Synthesizer

한국어로 구현하기가 가장 어려웠던 부분이었다. 

### Dataset
- Dataset: [AIHub 다화자 음성합성 데이터](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=data&dataSetSn=542)
	- Random으로 ZIP 파일 선택 후 다운로드 
	- Total Raw data size: 704GB
- Preprocessing
	- `korean_dataset.py`
		- Speaker별로 폴더 정리 
		- Extract Korean transcript
	- `korean_normalize.py`
		- preprocessing
		- normalizing

<img width="1015" alt="image" src="https://github.com/user-attachments/assets/16729a6f-73f9-4850-b098-c722bd4337eb">


```
Arguments:
    run_id:          su
    syn_dir:         /scratch4/dvd/GENERATION/SV2TTS/synthesizer/test
    models_dir:      /scratch3/dvd/GENERATION/result/saved_models
    save_every:      10
    backup_every:    5000
    force_restart:   False
    hparams:         

Checkpoint path: /scratch3/dvd/GENERATION/result/saved_models/su/synthesizer.pt
Loading training data from: /scratch4/dvd/GENERATION/SV2TTS/synthesizer/test/train.txt
Using model: Tacotron
Using device: cuda

Initialising Tacotron Model...
Trainable Parameters: 30.870M
Loading weights at /scratch3/dvd/GENERATION/result/saved_models/su/synthesizer.pt
Tacotron weights loaded from step 0
Using inputs from:
        /scratch4/dvd/GENERATION/SV2TTS/synthesizer/test/train.txt
        /scratch4/dvd/GENERATION/SV2TTS/synthesizer/test/mels
        /scratch4/dvd/GENERATION/SV2TTS/synthesizer/test/embeds
Found 270502 samples
+----------------+------------+---------------+------------------+
| Steps with r=2 | Batch Size | Learning Rate | Outputs/Step (r) |
+----------------+------------+---------------+------------------+
|   20k Steps    |     12     |     0.001     |        2         |
+----------------+------------+---------------+------------------+
```

## STEP3: Vocoder
-  Dataset: STEP2의 결과 사용 

## Reference
- [CorentinJ/Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning)
- [esoyeon/KoreanTTS](https://github.com/esoyeon/KoreanTTS)
- [hccho2/Tacotron2-Wavenet-Korean-TTS](https://github.com/hccho2/Tacotron2-Wavenet-Korean-TTS)


# 2️⃣ Proposal & Implementation of Deepvoice Detection w/ Federated Learning 

## Dataset
- ASVspoof 2019
	- homepage: https://www.asvspoof.org/index2019.html
	- LA (Logical Access): 공격자가 시스템이나 네트워크를 통해 음성 합성 또는 변조를 시도하는 공격 방식
	-   PA (Physical Access): 공격자가 물리적으로 장치에 접근하여 음성을 변조하는 공격 방식
	- 딥보이스 보이스피싱은 LA에 더 가깝다고 판단하여 LA 데이터셋만 사용 
- Fake-or-Real (FoR)
	- kaggle: https://www.kaggle.com/datasets/mohammedabdeldayem/the-fake-or-real-dataset/data

총 257,865개의 audio sample을 사용하였다. 그중 fake sample은 164,977개, real sample은 92,888개이다. 

## Data Preprocessing
Dataset의 version을 2가지 만들었다. 첫 번째는 librosa로 load한 raw dataset이다. 최대 길이인 80,000으로 padding하였다. 두 번째는 Pretrained Wav2Vec을 feature extractor로 사용한 것이다. 이 경우 역시 최대 길이인 80,000으로 padding하였다.

```bash
$ python preprocess.py
[INFO] max_length: 80000 
[INFO] Audio loaded (257865, 80000)
[INFO] Audio padded with 80000 
[INFO] Feature extracted (257865, 80000)
```
## DF
Deepvoice detection task에서는 주로 transformer, Wav2Vec, MLP가 쓰인다. 그중에서 Deepvoice detection model은 1d CNN 기반인 Deep-Fingerprinting([DF](https://dl.acm.org/doi/10.1145/3243734.3243768))을 선택하였다. 이 모델을 선택한 이유는 크게 두 가지이다. 첫 번째는 DF가 Website Fingerprinting에서 매우 강력하기 때문에 이진 분류 역시 잘 할 것이라 판단했기 때문이다. 두 번째 이유는 기존에 자주 사용되는 모델보다 DF가 더 deep하기 때문에 경량화를 진행할 수 있기 때문이다. 학습을 위해 경량화를 적용해보고 싶었다.

<img width="367" alt="image" src="https://github.com/user-attachments/assets/d1ca8739-240f-4998-8bbb-7855e30f09dd">

DF의 구조는 위 그림과 같다. DF의 official implementation은 Tensorflow 기반이다. FL을 Pytorch 기반으로 진행할 것이기 때문에 통일성을 위해 DF를 Pytorch로 바꿔주었다. 

## Flower Framework

<img width="250" alt="image" src="https://github.com/user-attachments/assets/9185eeb6-6301-4c21-ab83-c63d9b002f91">

연합학습 framework인 [Flower](https://flower.ai/) 를 선택하였다. 

## Experiment

Non-FL Setting에서 결과가 가장 좋은 model의 weight를 FL Setting에 사용하려고 한다. 

### Non-FL Setting

아래 2가지 요소를 다르게 하며 실험하였다.

- Raw 또는 W2V embedding
- Input dimension of DF: 80000, 50000, 10000, 5000

### FL Setting


## Reference
- [DF official implementation (Tensorflow)](https://github.com/deep-fingerprinting/df)
- [Flower Tutorials 1~5](https://flower.ai/docs/framework/tutorial-series-what-is-federated-learning.html)

# What I learn

## Extra Trouble Shooting

### Unzip extremely large ZIP file
Synthesizer에서 사용한 다화자 음성합성 데이터는 각 40~50GB의 ZIP 파일 여러 개로 이루어져 있다. 각각의 ZIP 파일을 `unzip` 명령어로 압축 해제하려 하면 다음과 같은 에러가 뜬다.

```bash
$ unzip VS4.zip -d VS4 
Archive: VS4.zip 
warning [VS4.zip]: 48227474065 extra bytes at beginning or within zipfile
	(attempting to process anyway) 
error [VS4.zip]: start of central directory not found; zipfile corrupt.
	(please check that you have transferred or created the zipfile in the appropriate BINARY mode and that you have compiled UnZip properly)
```
처음에는 서버에 ZIP 파일이 제대로 업로드되지 않아서 이 에러가 발생했다고 생각했다. 하지만 서버에 잘 올라간 걸 분명히 확인한 경우에도 그대로라 다른 방법을 찾기 시작했다. 

그 결과 [이 블로그](https://kaen2891.tistory.com/76)를 보고 대용량 ZIP 파일 압축 해제 시 발생한다는 것을 알았다. 

```
$ zip -FF TS17.zip --out TS17_2.zip
```
위와 같은 명령어로 생성된 ZIP 파일을 `unzip` 해주면 정상적으로 압축해제된다. 

보통 에러가 발생하면 바로 검색하는데 local - server 업로드에서 잘못 되었다고 생각해 안 찾아봤던 것이 화근이었다. 

### 하위 파일 개수가 매우 많은 폴더를 vs code explorer로 연 이후  ssh remote open에 실패하는 경우

vs code에서 하단 `output` 을 누르면 아래와 같은 메시지가 보인다.
```
[10:21:53.924] [server] Checking /.vscode-server/cli/servers/Stable-f1a4fb101478ce6ecxxxxxxxxxxxxxxxx/log.txt and
 /.vscode-server/cli/servers/Stable-f1a4fb101478ce6ecxxxxxxxxxxxxxxxx/pid.txt for a running server... 
[10:21:53.927] [server] Found running server (pid=2823647)
```

- 1️⃣ 여기서 `log.txt`와 `pid.txt` 경로를 복사한다.
- 2️⃣ vs code가 아닌 local의 command prompt로 ssh 접속한다.
- 3️⃣`rm` 명령어로 복사한 두 `.txt` 파일을 삭제한다.

vs code로 재접속했을 때 이전에 해당 폴더를 잘못 연 connection 정보가 사라지게 되어 새로운 connection이 열리게 된다.

