---
title: Deep Learning Specialization 2-7,8 | Batch Normalization
author: Su
date: 2023-12-18 03:55:00 +0800
categories: [DL]
tags: [EURON]
pin: false
use_math: true
---

Learning Source
+ [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning?utm_source=deeplearningai&utm_medium=institutions&utm_campaign=SocialYoutubeDLSC1W1L1#courses)
+ [부스트코스 딥러닝 2단계: 심층 신경망 성능 향상시키기](https://m.boostcourse.org/ai216/lectures/132205)

<br>

## **Softmax Regression**
<img width="637" alt="image" src="https://github.com/sml09181/sml09181.github.io/assets/105408672/25047520-6f59-4560-8c0b-3cbb8d5406d7">

+ 여러 개의 클래스 분류시 사용
+ 출력 레이어 이후에 적용
+ 마지막 층의 출력값이 주어졌을 때 해당 클래스에 속할 확률을 Softmax 층을 통해서 구할 수 있다. 마지막 선형 출력값($z$)들을 각각 지수화시켜 임시변수 $t=e^{z}$ 를 만든다. 그후 모든 값들의 합이 1이 될 수 있도록 모든 임시 변수값들의 합을 나눠서 정규화시킨다.
+ $a_i = \frac{e_{z_i}}{\sum_{j=1}^Ce^{z_j}}$ 
<br>

<img width="635" alt="image" src="https://github.com/sml09181/sml09181.github.io/assets/105408672/064e8e29-d1a7-4bb9-92c9-2f43bd8cf5e8">

+ 두 클래스 사이의 경계가 선형이다.


### 손실함수

+ $L(\hat{y}, y) = -\displaystyle{\sum_{j=1}^4} y_j \log y_j $
  + $y = [0, 1, 0, 0]$
  + $\hat{y} = [0.3, 0.2, 0.1, 0.4]$
+ 두 번째를 제외한 나머지 $y_j$ 은 0이기 때문에 $-\log(y_2)$ 값만 남는다.
+ 즉 이 값을 최소화 하여 클래스 2이 될 확률을 최대화시킨다.
+ Softmax 와 손실함수를 결합한 역전파 값
  + $dz^{[L]} = \hat{y} - y$
+ Softmax vs Hardmax
  + Softmax: converts a vector of K real numbers into a probability distribution of K possible outcomes(Wiki)
  + Hardmax: $z$ 값 중 가장 큰 값이 있는 곳에 1을, 나머지에는 0을 갖는 벡터로 대응시키는 것

## **Deep Learning Frameworks**
+ Deep Learning Frameworks: 일반적으로 Python과 같은 하위 수준 언어보다 적은 코드로 딥러닝 알고리즘을 작성할 수 있게 해 준다.<br>

<img width="631" alt="image" src="https://github.com/sml09181/sml09181.github.io/assets/105408672/52783571-626b-4410-97cc-0dcd45638dbe">


## **지역 최적값 문제**
+ 고차원 비용함수에서 경사가 0인 경우는 대부분 지역 최적값이 아니라 대개 안장점이다.
+ 안장지대
  + 안장점으로 향하는 구간
  + 미분값이 아주 오랫동안 0에 가깝게 유지되는 지역
+ 대개 충분히 큰 Network 학습시 지역 최적값에 갇히는 일은 거의 없다.
+ 안장지대의 문제점
  + 경사가 거의 0에 가깝기 때문에 학습속도가 느리다. 
  + 다른 쪽으로 방향변환이 없다면 안정지대에서 벗어나기 어렵다.
  + 이는 Adam, RMSprop 등 알고리즘을 사용하면 해소된다.


## **Tensorflow**

```python
  coefficients = np.array([[1.], [-20], [100]])
  w = tf.Variable(0, dtype=tf.float32)
  x = tf.Variable(coefficients, dtype=tf.float32) # x = cofficients

  def cost_function():
    return x[0][0]*w**2 + x[1][0]*w + x[2][0]

  optimizer = tf.optimizers.SGD(0.01)
  optimizer.minimize(cost_function, var_list=[w])

  print(w.numpy())
```